{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b179de08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-10T10:55:14.786544Z",
          "iopub.status.busy": "2025-12-10T10:55:14.785963Z",
          "iopub.status.idle": "2025-12-10T10:55:16.404524Z",
          "shell.execute_reply": "2025-12-10T10:55:16.403884Z",
          "shell.execute_reply.started": "2025-12-10T10:55:14.786516Z"
        },
        "id": "b179de08",
        "outputId": "b6bc6b60-d8d7-4ead-d3c6-6dea70441f51",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: python-chess in /usr/local/lib/python3.12/dist-packages (1.999)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: chess<2,>=1 in /usr/local/lib/python3.12/dist-packages (from python-chess) (1.11.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "‚úÖ Device: cuda\n",
            "üìÇ Model path (read): /content/drive/MyDrive/models\n",
            "üìÇ Save path (write): /content/drive/MyDrive/models\n",
            "üîç Logic Check: Final Index reached = 4672\n",
            "üîí Canonical move map initialized: 1924 moves (Chu·∫©n AlphaZero)\n"
          ]
        }
      ],
      "source": [
        "# [CELL 0] Setup Environment & Install Dependencies\n",
        "!pip install torch numpy python-chess tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import chess\n",
        "import os\n",
        "import pickle\n",
        "from collections import deque\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Ki·ªÉm tra GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚úÖ Device: {device}\")\n",
        "if device.type == 'cpu':\n",
        "    print(\"‚ö†Ô∏è Warning: ƒêang ch·∫°y tr√™n CPU. H√£y b·∫≠t GPU T4 trong Runtime > Change runtime type\")\n",
        "\n",
        "# =========================\n",
        "# Global Config (easy to edit)\n",
        "# =========================\n",
        "MODEL_PATH = '/content/drive/MyDrive/models'            # Path to read existing models/resources\n",
        "SAVE_PATH = '/content/drive/MyDrive/models'            # Path to save new checkpoints/buffers/logs\n",
        "MODEL_NAME_BEST = 'model_rl_best.pth'\n",
        "SFT_MODEL_NAME = 'model_sft.pth'\n",
        "BUFFER_NAME_LATEST = 'buffer_latest.pkl'\n",
        "BUFFER_NAME_RL_LATEST = 'buffer_rl_latest.pkl'\n",
        "TRAINING_LOG_NAME = 'training_log.csv'\n",
        "\n",
        "# Derived absolute paths\n",
        "MODEL_BEST_PATH = os.path.join(SAVE_PATH, MODEL_NAME_BEST)\n",
        "SFT_MODEL_PATH = os.path.join(MODEL_PATH, SFT_MODEL_NAME)\n",
        "BUFFER_LATEST_PATH = os.path.join(SAVE_PATH, BUFFER_NAME_LATEST)\n",
        "BUFFER_RL_LATEST_PATH = os.path.join(SAVE_PATH, BUFFER_NAME_RL_LATEST)\n",
        "TRAINING_LOG_PATH = os.path.join(SAVE_PATH, TRAINING_LOG_NAME)\n",
        "MODEL_LATEST_PATH = os.path.join(SAVE_PATH, 'model_rl_latest.pth')\n",
        "MOVE_MAP_LATEST_PATH = os.path.join(SAVE_PATH, 'move_map.pkl')\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(MODEL_PATH, exist_ok=True)\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "print(f\"üìÇ Model path (read): {MODEL_PATH}\")\n",
        "print(f\"üìÇ Save path (write): {SAVE_PATH}\")\n",
        "\n",
        "# =========================\n",
        "# Canonical 4672 Move Map (Hard-coded AlphaZero-style)\n",
        "# =========================\n",
        "def build_canonical_move_map_4672():\n",
        "    move_to_idx = {}\n",
        "    idx_to_move = {}\n",
        "    idx = 0\n",
        "\n",
        "    # 1. Queen Moves (8 h∆∞·ªõng * 7 √¥) = 3584 indices\n",
        "    directions = [(1, 0), (1, 1), (0, 1), (-1, 1), (-1, 0), (-1, -1), (0, -1), (1, -1)]\n",
        "    for from_sq in range(64):\n",
        "        for d_r, d_f in directions:\n",
        "            for dist in range(1, 8):\n",
        "                to_rank = (from_sq // 8) + d_r * dist\n",
        "                to_file = (from_sq % 8) + d_f * dist\n",
        "                if 0 <= to_rank < 8 and 0 <= to_file < 8:\n",
        "                    dest = to_rank * 8 + to_file\n",
        "                    uci = chess.Move(from_sq, dest).uci()\n",
        "                    move_to_idx[uci] = idx\n",
        "                    idx_to_move[idx] = uci\n",
        "                idx += 1\n",
        "\n",
        "    # 2. Knight Moves (8 h∆∞·ªõng) = 512 indices\n",
        "    knight_moves = [(2, 1), (1, 2), (-1, 2), (-2, 1), (-2, -1), (-1, -2), (1, -2), (2, -1)]\n",
        "    for from_sq in range(64):\n",
        "        for d_r, d_f in knight_moves:\n",
        "            to_rank = (from_sq // 8) + d_r\n",
        "            to_file = (from_sq % 8) + d_f\n",
        "            if 0 <= to_rank < 8 and 0 <= to_file < 8:\n",
        "                dest = to_rank * 8 + to_file\n",
        "                uci = chess.Move(from_sq, dest).uci()\n",
        "                move_to_idx[uci] = idx\n",
        "                idx_to_move[idx] = uci\n",
        "            idx += 1\n",
        "\n",
        "    # 3. Underpromotions (3 h∆∞·ªõng * 3 lo·∫°i qu√¢n) = 576 indices\n",
        "    file_steps = [0, -1, 1]\n",
        "    promotions = ['r', 'b', 'n']\n",
        "\n",
        "    for from_sq in range(64):\n",
        "        rank = from_sq // 8\n",
        "        file = from_sq % 8\n",
        "\n",
        "        # X√°c ƒë·ªãnh h∆∞·ªõng phong c·∫•p d·ª±a tr√™n v·ªã tr√≠ qu√¢n t·ªët\n",
        "        rank_step = 0\n",
        "        if rank == 6:\n",
        "            rank_step = 1  # Tr·∫Øng: L√™n\n",
        "        elif rank == 1:\n",
        "            rank_step = -1 # ƒêen: Xu·ªëng\n",
        "\n",
        "        for f_step in file_steps:\n",
        "            for p in promotions:\n",
        "                if rank_step != 0:\n",
        "                    to_rank = rank + rank_step\n",
        "                    to_file = file + f_step\n",
        "                    if 0 <= to_file < 8:\n",
        "                        dest = to_rank * 8 + to_file\n",
        "                        uci = chess.Move(from_sq, dest, promotion=chess.Piece.from_symbol(p).piece_type).uci()\n",
        "                        move_to_idx[uci] = idx\n",
        "                        idx_to_move[idx] = uci\n",
        "                idx += 1\n",
        "    return move_to_idx, idx_to_move\n",
        "\n",
        "# Kh·ªüi t·∫°o l·∫°i\n",
        "CANONICAL_MOVE_TO_IDX, CANONICAL_IDX_TO_MOVE = build_canonical_move_map_4672()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "638359a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-10T10:55:16.405968Z",
          "iopub.status.busy": "2025-12-10T10:55:16.405663Z",
          "iopub.status.idle": "2025-12-10T10:55:16.416099Z",
          "shell.execute_reply": "2025-12-10T10:55:16.415439Z",
          "shell.execute_reply.started": "2025-12-10T10:55:16.405950Z"
        },
        "id": "638359a2",
        "outputId": "4cdead8f-ebd6-427e-fc93-fc4c6e6bfea9",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ SmallResNet defined\n"
          ]
        }
      ],
      "source": [
        "#Define Model Architecture - SmallResNet\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, num_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, stride=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, stride=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += residual\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class SmallResNet(nn.Module):\n",
        "    def __init__(self, num_res_blocks=6, num_channels=64, action_size=4672):\n",
        "        super(SmallResNet, self).__init__()\n",
        "        # Input: 32 channels (Current + History + Aux)\n",
        "        self.conv_input = nn.Conv2d(32, num_channels, kernel_size=3, padding=1, stride=1, bias=False)\n",
        "        self.bn_input = nn.BatchNorm2d(num_channels)\n",
        "\n",
        "        # Backbone: Residual Tower\n",
        "        self.res_blocks = nn.ModuleList([\n",
        "            ResidualBlock(num_channels) for _ in range(num_res_blocks)\n",
        "        ])\n",
        "\n",
        "        # Policy Head (Actor)\n",
        "        self.policy_conv = nn.Conv2d(num_channels, 32, kernel_size=1, stride=1, bias=False)\n",
        "        self.policy_bn = nn.BatchNorm2d(32)\n",
        "        self.policy_fc = nn.Linear(32 * 8 * 8, action_size)\n",
        "\n",
        "        # Value Head (Critic)\n",
        "        self.value_conv = nn.Conv2d(num_channels, 3, kernel_size=1, stride=1, bias=False)\n",
        "        self.value_bn = nn.BatchNorm2d(3)\n",
        "        self.value_fc1 = nn.Linear(3 * 8 * 8, 64)\n",
        "        self.value_fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn_input(self.conv_input(x)))\n",
        "        for block in self.res_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Policy: Softmax\n",
        "        p = F.relu(self.policy_bn(self.policy_conv(x)))\n",
        "        p = p.view(-1, 32 * 8 * 8)\n",
        "        policy_logits = self.policy_fc(p)  # Tr·∫£ v·ªÅ RAW LOGITS\n",
        "\n",
        "        # Value Head\n",
        "        v = F.relu(self.value_bn(self.value_conv(x)))\n",
        "        v = v.view(-1, 3 * 8 * 8)\n",
        "        v = F.relu(self.value_fc1(v))\n",
        "        v = torch.tanh(self.value_fc2(v))\n",
        "\n",
        "        return policy_logits, v  # Tr·∫£ v·ªÅ Logits\n",
        "print(\"‚úÖ SmallResNet defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e57a819",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-10T10:55:16.416933Z",
          "iopub.status.busy": "2025-12-10T10:55:16.416756Z",
          "iopub.status.idle": "2025-12-10T10:55:16.436105Z",
          "shell.execute_reply": "2025-12-10T10:55:16.435403Z",
          "shell.execute_reply.started": "2025-12-10T10:55:16.416918Z"
        },
        "id": "3e57a819",
        "outputId": "7d3af56f-0837-4820-a7e0-9b16e66f4f07",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ChessConverter defined (hard-coded canonical map, with queen-promotion fallback)\n"
          ]
        }
      ],
      "source": [
        "#ChessConverter - Board encoding & Move mapping (Hard-coded)\n",
        "class ChessConverter:\n",
        "    def __init__(self, move_to_idx=None, idx_to_move=None):\n",
        "        self.piece_map = {\n",
        "            'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
        "            'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
        "        }\n",
        "        # Use fixed canonical map; no dynamic additions\n",
        "        self.move_to_idx = move_to_idx if move_to_idx is not None else CANONICAL_MOVE_TO_IDX\n",
        "        self.idx_to_move = idx_to_move if idx_to_move is not None else CANONICAL_IDX_TO_MOVE\n",
        "        self.next_idx = len(self.move_to_idx)  # informational only\n",
        "\n",
        "    def encode_move(self, move_uci):\n",
        "        # Hard-coded: unknown moves are ignored\n",
        "        idx = self.move_to_idx.get(move_uci, None)\n",
        "        if idx is not None:\n",
        "            return idx\n",
        "        # This preserves the 4672 action space and supports common promotions.\n",
        "        if isinstance(move_uci, str) and len(move_uci) == 5 and move_uci[-1] == 'q':\n",
        "            base_uci = move_uci[:4]\n",
        "            return self.move_to_idx.get(base_uci, None)\n",
        "        return None\n",
        "\n",
        "    def board_to_tensor(self, board, prev_board=None):\n",
        "        tensor = np.zeros((32, 8, 8), dtype=np.float32)\n",
        "        for sq, pc in board.piece_map().items():\n",
        "            tensor[self.piece_map[pc.symbol()]][chess.square_rank(sq)][chess.square_file(sq)] = 1\n",
        "        if prev_board:\n",
        "            for sq, pc in prev_board.piece_map().items():\n",
        "                tensor[self.piece_map[pc.symbol()]+12][chess.square_rank(sq)][chess.square_file(sq)] = 1\n",
        "        if board.turn == chess.WHITE: tensor[24,:,:] = 1\n",
        "        if board.has_kingside_castling_rights(chess.WHITE): tensor[25,:,:] = 1\n",
        "        if board.has_queenside_castling_rights(chess.WHITE): tensor[26,:,:] = 1\n",
        "        if board.has_kingside_castling_rights(chess.BLACK): tensor[27,:,:] = 1\n",
        "        if board.has_queenside_castling_rights(chess.BLACK): tensor[28,:,:] = 1\n",
        "        if board.ep_square:\n",
        "            tensor[29][chess.square_rank(board.ep_square)][chess.square_file(board.ep_square)] = 1\n",
        "        if board.is_repetition(1): tensor[30,:,:] = 1\n",
        "        if board.is_repetition(2): tensor[31,:,:] = 1\n",
        "        return tensor\n",
        "\n",
        "print(\"‚úÖ ChessConverter defined (hard-coded canonical map, with queen-promotion fallback)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "032a7a48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-10T10:55:16.437987Z",
          "iopub.status.busy": "2025-12-10T10:55:16.437758Z",
          "iopub.status.idle": "2025-12-10T10:55:16.458630Z",
          "shell.execute_reply": "2025-12-10T10:55:16.458103Z",
          "shell.execute_reply.started": "2025-12-10T10:55:16.437971Z"
        },
        "id": "032a7a48",
        "outputId": "d8ac5c39-fae8-48cc-c117-236c2d5d8515",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ MCTS defined (fixed terminal value: side to move loses = -1)\n"
          ]
        }
      ],
      "source": [
        "# MCTS - Monte Carlo Tree Search\n",
        "class MCTSNode:\n",
        "    def __init__(self, parent=None, move=None, prior=0.0):\n",
        "        self.parent = parent\n",
        "        self.move = move\n",
        "        self.prior = prior\n",
        "        self.children = {}\n",
        "        self.visit_count = 0\n",
        "        self.value_sum = 0.0\n",
        "\n",
        "    def value(self):\n",
        "        return self.value_sum / self.visit_count if self.visit_count > 0 else 0.0\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return len(self.children) == 0\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, model, device, c_puct=1.5, dirichlet_alpha=0.3, dirichlet_epsilon=0.25, fpu_mode='zero'):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.c_puct = c_puct\n",
        "        self.converter = ChessConverter()\n",
        "        # Root Dirichlet noise params\n",
        "        self.dirichlet_alpha = dirichlet_alpha\n",
        "        self.dirichlet_epsilon = dirichlet_epsilon\n",
        "        # First Play Urgency: 'zero' or 'parent'\n",
        "        self.fpu_mode = fpu_mode\n",
        "\n",
        "    def search(self, board, num_simulations=10, temperature=1.0, prev_board=None, root_noise=True):\n",
        "        root = MCTSNode()\n",
        "        search_board = board.copy()\n",
        "        initial_prev_board = prev_board\n",
        "\n",
        "        # Expand root once and add Dirichlet noise to priors\n",
        "        search_board.set_fen(board.fen())\n",
        "        root_value = self._expand_and_evaluate(root, search_board, initial_prev_board)\n",
        "        if root_noise and len(root.children) > 0:\n",
        "            noise = np.random.dirichlet([self.dirichlet_alpha] * len(root.children))\n",
        "            for (move, child), n in zip(root.children.items(), noise):\n",
        "                child.prior = (1.0 - self.dirichlet_epsilon) * child.prior + self.dirichlet_epsilon * n\n",
        "\n",
        "        for _ in range(num_simulations):\n",
        "            node = root\n",
        "            search_board.set_fen(board.fen())\n",
        "            prev_board = initial_prev_board\n",
        "\n",
        "            while not node.is_leaf() and not search_board.is_game_over():\n",
        "                node = self._select_child(node, search_board)\n",
        "                if node.move:\n",
        "                    # track previous board before applying move\n",
        "                    prev_for_next = search_board.copy()\n",
        "                    search_board.push(node.move)\n",
        "                    prev_board = prev_for_next\n",
        "\n",
        "            if not search_board.is_game_over():\n",
        "                value = self._expand_and_evaluate(node, search_board, prev_board)\n",
        "            else:\n",
        "                # Terminal value from the perspective of the side to move\n",
        "                result = search_board.result()\n",
        "                if result == \"1-0\":\n",
        "                    value = -1.0 if search_board.turn == chess.BLACK else 1.0\n",
        "                elif result == \"0-1\":\n",
        "                    value = -1.0 if search_board.turn == chess.WHITE else 1.0\n",
        "                else:\n",
        "                    value = 0.0\n",
        "\n",
        "            self._backpropagate(node, value)\n",
        "\n",
        "        return self._get_action_probs(root, board, temperature)\n",
        "\n",
        "    def _select_child(self, node, board):\n",
        "        best_score = -float('inf')\n",
        "        best_child = None\n",
        "\n",
        "        for move, child in node.children.items():\n",
        "            if move not in board.legal_moves:\n",
        "                continue\n",
        "            # FPU handling: if unvisited, use parent value or zero\n",
        "            if child.visit_count == 0:\n",
        "                q_value = node.value() if self.fpu_mode == 'parent' else 0.0\n",
        "            else:\n",
        "                q_value = - child.value()\n",
        "            u_value = self.c_puct * child.prior * np.sqrt(max(1, node.visit_count)) / (1 + child.visit_count)\n",
        "            score = q_value + u_value\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_child = child\n",
        "        return best_child if best_child else node\n",
        "\n",
        "    def _expand_and_evaluate(self, node, board, prev_board):\n",
        "        state = self.converter.board_to_tensor(board, prev_board=prev_board).reshape(1, 32, 8, 8)\n",
        "        state_t = torch.FloatTensor(state).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            policy_logits, value = self.model(state_t)\n",
        "\n",
        "        #raw probability t·ª´ to√†n b·ªô 4672 ƒë·∫ßu ra\n",
        "        policy = torch.softmax(policy_logits, dim=1).cpu().numpy()[0]\n",
        "        value = value.cpu().item()\n",
        "\n",
        "        #Normalize ch·ªâ tr√™n c√°c n∆∞·ªõc ƒëi h·ª£p l·ªá\n",
        "        legal_moves = list(board.legal_moves)\n",
        "        move_probs = []\n",
        "        total_prob = 0.0\n",
        "\n",
        "        for move in legal_moves:\n",
        "            move_idx = self.converter.encode_move(move.uci())\n",
        "            if move_idx is not None:\n",
        "                prob = policy[move_idx] # L·∫•y prob c·ªßa n∆∞·ªõc ƒëi n√†y\n",
        "            else:\n",
        "                prob = 0.0\n",
        "            move_probs.append(prob)\n",
        "            total_prob += prob\n",
        "\n",
        "        # G√°n Prior ƒë√£ chu·∫©n h√≥a v√†o Node con\n",
        "        for move, prob in zip(legal_moves, move_probs):\n",
        "            if total_prob > 0:\n",
        "                prior = prob / total_prob \n",
        "            else:\n",
        "                prior = 1.0 / len(legal_moves) # Fallback n·∫øu model output to√†n 0\n",
        "\n",
        "            node.children[move] = MCTSNode(parent=node, move=move, prior=prior)\n",
        "\n",
        "        return value\n",
        "\n",
        "    def _backpropagate(self, node, value):\n",
        "        while node is not None:\n",
        "            node.visit_count += 1\n",
        "            node.value_sum += value\n",
        "            value = -value\n",
        "            node = node.parent\n",
        "\n",
        "    def _get_action_probs(self, root, board, temperature):\n",
        "        action_probs = np.zeros(4672, dtype=np.float32)\n",
        "        visits = []\n",
        "        moves = []\n",
        "\n",
        "        for move, child in root.children.items():\n",
        "            if move in board.legal_moves:\n",
        "                visits.append(child.visit_count)\n",
        "                moves.append(move)\n",
        "\n",
        "        if len(visits) == 0:\n",
        "            return action_probs\n",
        "\n",
        "        visits = np.array(visits, dtype=np.float32)\n",
        "        if temperature == 0:\n",
        "            probs = np.zeros(len(visits))\n",
        "            probs[np.argmax(visits)] = 1.0\n",
        "        else:\n",
        "            visits = visits ** (1.0 / temperature)\n",
        "            probs = visits / visits.sum()\n",
        "\n",
        "        for move, prob in zip(moves, probs):\n",
        "            move_idx = self.converter.encode_move(move.uci())\n",
        "            if move_idx is not None:\n",
        "                action_probs[move_idx] = prob\n",
        "\n",
        "        return action_probs\n",
        "\n",
        "print(\"‚úÖ MCTS defined (fixed terminal value: side to move loses = -1)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cbb3f899",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-10T10:55:16.460125Z",
          "iopub.status.busy": "2025-12-10T10:55:16.459895Z",
          "iopub.status.idle": "2025-12-10T10:55:16.478317Z",
          "shell.execute_reply": "2025-12-10T10:55:16.477754Z",
          "shell.execute_reply.started": "2025-12-10T10:55:16.460109Z"
        },
        "id": "cbb3f899",
        "outputId": "933bf766-4202-4e3f-becd-72cd45b41570",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ReplayBuffer defined\n"
          ]
        }
      ],
      "source": [
        "# [CELL 4] ReplayBuffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=100000):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def add_batch(self, samples):\n",
        "        self.buffer.extend(samples)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def save(self, path):\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(list(self.buffer), f)\n",
        "        print(f\"üíæ Buffer saved: {len(self.buffer)} samples\")\n",
        "\n",
        "    def load(self, path):\n",
        "        if os.path.exists(path):\n",
        "            with open(path, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "                self.buffer = deque(data, maxlen=self.buffer.maxlen)\n",
        "            print(f\"üìñ Buffer loaded: {len(self.buffer)} samples\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "print(\"‚úÖ ReplayBuffer defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4d2ff94",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-10T10:55:16.479504Z",
          "iopub.status.busy": "2025-12-10T10:55:16.479275Z",
          "iopub.status.idle": "2025-12-10T10:55:16.497872Z",
          "shell.execute_reply": "2025-12-10T10:55:16.497212Z",
          "shell.execute_reply.started": "2025-12-10T10:55:16.479483Z"
        },
        "id": "d4d2ff94",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# AlphaZeroTrainer\n",
        "class AlphaZeroTrainer:\n",
        "    def __init__(self, model, device=device, lr=0.001, weight_decay=1e-4):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=20, gamma=0.5)\n",
        "        self.replay_buffer = ReplayBuffer(max_size=100000)\n",
        "        self.converter = ChessConverter()\n",
        "        self.use_self_play = False\n",
        "        self.mcts = MCTS(model=self.model, device=device, c_puct=1.0)\n",
        "\n",
        "    def combined_loss(self, policy_pred, value_pred, policy_target, value_target):\n",
        "        # Policy loss: cross-entropy with log-softmax\n",
        "        log_policy = torch.log_softmax(policy_pred, dim=1)\n",
        "        policy_loss = -(policy_target * log_policy).sum(dim=1).mean()\n",
        "        value_loss = nn.MSELoss()(value_pred, value_target)\n",
        "        return policy_loss + value_loss, policy_loss, value_loss\n",
        "\n",
        "    def _get_ai_policy(self, board, prev_board=None):\n",
        "        state = self.converter.board_to_tensor(board, prev_board=prev_board).reshape(1, 32, 8, 8)\n",
        "        state_t = torch.FloatTensor(state).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            policy_logits, _ = self.model(state_t)\n",
        "        policy_probs = torch.softmax(policy_logits, dim=1).cpu().numpy()[0]\n",
        "        return policy_probs\n",
        "\n",
        "    def train_on_batch(self, batch):\n",
        "        self.model.train()\n",
        "        states, policies, values = zip(*batch)\n",
        "        states = torch.stack([torch.FloatTensor(s) for s in states]).to(self.device)\n",
        "        policies = torch.stack([torch.FloatTensor(p) for p in policies]).to(self.device)\n",
        "        values = torch.FloatTensor(values).unsqueeze(1).to(self.device)\n",
        "\n",
        "        policy_pred, value_pred = self.model(states)\n",
        "        loss, policy_loss, value_loss = self.combined_loss(policy_pred, value_pred, policies, values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item(), policy_loss.item(), value_loss.item()\n",
        "\n",
        "    def save_checkpoint(self, path):\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'use_self_play': self.use_self_play,\n",
        "        }, path)\n",
        "        print(f\"üíæ Saved: {path}\")\n",
        "\n",
        "    def load_checkpoint(self, path):\n",
        "        checkpoint = torch.load(path, map_location=self.device)\n",
        "        state_dict = checkpoint['model_state_dict'] if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint else checkpoint\n",
        "        # Detect policy head size from checkpoint\n",
        "        policy_key_w = 'policy_fc.weight'\n",
        "        policy_key_b = 'policy_fc.bias'\n",
        "        if policy_key_w in state_dict:\n",
        "            target_action_size = state_dict[policy_key_w].shape[0]\n",
        "            current_action_size = self.model.policy_fc.out_features\n",
        "            if target_action_size != current_action_size:\n",
        "                print(f\"‚öôÔ∏è Rebuilding model: checkpoint outputs={target_action_size}, current={current_action_size}\")\n",
        "                # Recreate model with the target action size\n",
        "                new_model = SmallResNet(num_res_blocks=6, num_channels=64, action_size=target_action_size).to(self.device)\n",
        "                # Replace trainer model and MCTS model reference\n",
        "                self.model = new_model\n",
        "                if hasattr(self, 'mcts') and self.mcts is not None:\n",
        "                    self.mcts.model = self.model\n",
        "                # Recreate optimizer tied to new parameters\n",
        "                self.optimizer = optim.Adam(self.model.parameters(), lr=self.optimizer.param_groups[0]['lr'], weight_decay=self.optimizer.param_groups[0].get('weight_decay', 1e-4))\n",
        "                # Recreate scheduler\n",
        "                self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=20, gamma=0.5)\n",
        "        # Load weights\n",
        "        self.model.load_state_dict(state_dict)\n",
        "        if isinstance(checkpoint, dict) and 'optimizer_state_dict' in checkpoint:\n",
        "            try:\n",
        "                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Optimizer state mismatch; reinitialized. Details: {e}\")\n",
        "        # Restore scheduler state if present\n",
        "        if isinstance(checkpoint, dict) and 'scheduler_state_dict' in checkpoint:\n",
        "            try:\n",
        "                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Scheduler state mismatch; reinitialized. Details: {e}\")\n",
        "        if isinstance(checkpoint, dict) and 'use_self_play' in checkpoint:\n",
        "            self.use_self_play = checkpoint['use_self_play']\n",
        "        print(f\"üìñ Loaded: {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f62a9986",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-10T10:55:16.516539Z",
          "iopub.status.busy": "2025-12-10T10:55:16.516331Z",
          "iopub.status.idle": "2025-12-10T10:55:16.535376Z",
          "shell.execute_reply": "2025-12-10T10:55:16.534790Z",
          "shell.execute_reply.started": "2025-12-10T10:55:16.516514Z"
        },
        "id": "f62a9986",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# AlphaZeroTrainer - Game playing methods\n",
        "def self_play_game(self, num_simulations=10, temperature=1.0):\n",
        "    board = chess.Board()\n",
        "    samples = []\n",
        "    move_count = 0\n",
        "    ai_plays_white = random.choice([True, False])\n",
        "    prev_board = None\n",
        "\n",
        "    def _safe_sample_move(legal_moves, probs_like):\n",
        "        # Convert and sanitize\n",
        "        probs = np.array(probs_like, dtype=np.float64)\n",
        "        probs = np.nan_to_num(probs, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        # Clip negatives and renormalize\n",
        "        probs = np.clip(probs, 0.0, None)\n",
        "        s = probs.sum()\n",
        "        if s <= 0.0:\n",
        "            return random.choice(legal_moves)\n",
        "        probs /= s\n",
        "        # Adjust last element to fix any floating drift\n",
        "        if len(probs) > 0:\n",
        "            drift = 1.0 - probs.sum()\n",
        "            probs[-1] += drift\n",
        "            # Final safety clip and renorm\n",
        "            probs = np.clip(probs, 0.0, None)\n",
        "            s = probs.sum()\n",
        "            if s <= 0.0:\n",
        "                return random.choice(legal_moves)\n",
        "            probs /= s\n",
        "        try:\n",
        "            idx = np.random.choice(len(legal_moves), p=probs)\n",
        "            return legal_moves[idx]\n",
        "        except ValueError:\n",
        "            # Fallback to uniform if numpy rejects p\n",
        "            return random.choice(legal_moves)\n",
        "\n",
        "    while not board.is_game_over() and move_count < 200:\n",
        "        state = self.converter.board_to_tensor(board, prev_board=prev_board)\n",
        "        current_turn_is_white = board.turn == chess.WHITE\n",
        "\n",
        "        if self.use_self_play:\n",
        "            temp = 1.0 if move_count < 30 else 0.0\n",
        "            policy_target = self.mcts.search(board, num_simulations=num_simulations, temperature=temp, prev_board=prev_board, root_noise=True)\n",
        "\n",
        "            legal_moves = list(board.legal_moves)\n",
        "            legal_probs = [policy_target[self.converter.encode_move(m.uci())] if self.converter.encode_move(m.uci()) is not None else 0.0 for m in legal_moves]\n",
        "\n",
        "            if sum(legal_probs) > 0:\n",
        "                selected_move = _safe_sample_move(legal_moves, legal_probs)\n",
        "            else:\n",
        "                selected_move = random.choice(legal_moves)\n",
        "\n",
        "            samples.append((state, policy_target, None, current_turn_is_white))\n",
        "            prev_for_next = board.copy()\n",
        "            board.push(selected_move)\n",
        "            prev_board = prev_for_next\n",
        "        else:\n",
        "            if (board.turn == chess.WHITE) == ai_plays_white:\n",
        "                policy = self._get_ai_policy(board, prev_board=prev_board)\n",
        "                legal_moves = list(board.legal_moves)\n",
        "                legal_probs = [policy[self.converter.encode_move(m.uci())] if self.converter.encode_move(m.uci()) is not None else 0.0 for m in legal_moves]\n",
        "\n",
        "                if sum(legal_probs) > 0:\n",
        "                    selected_move = _safe_sample_move(legal_moves, legal_probs)\n",
        "                else:\n",
        "                    selected_move = random.choice(legal_moves)\n",
        "\n",
        "                policy_target = np.zeros(4672, dtype=np.float32)\n",
        "                move_idx = self.converter.encode_move(selected_move.uci())\n",
        "                if move_idx is not None:\n",
        "                    policy_target[move_idx] = 1.0\n",
        "\n",
        "                samples.append((state, policy_target, None, current_turn_is_white))\n",
        "                prev_for_next = board.copy()\n",
        "                board.push(selected_move)\n",
        "                prev_board = prev_for_next\n",
        "            else:\n",
        "                prev_for_next = board.copy()\n",
        "                board.push(random.choice(list(board.legal_moves)))\n",
        "                prev_board = prev_for_next\n",
        "\n",
        "        move_count += 1\n",
        "\n",
        "    result = board.result()\n",
        "    white_outcome = 1.0 if result == \"1-0\" else (-1.0 if result == \"0-1\" else -0.1)\n",
        "\n",
        "    final_samples = []\n",
        "    for state, policy, _, was_white_turn in samples:\n",
        "        value = white_outcome if was_white_turn else -white_outcome\n",
        "        final_samples.append((state, policy, value))\n",
        "\n",
        "    return final_samples, white_outcome, ai_plays_white if not self.use_self_play else None\n",
        "\n",
        "\n",
        "AlphaZeroTrainer.self_play_game = self_play_game"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4b077fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-10T10:55:16.536380Z",
          "iopub.status.busy": "2025-12-10T10:55:16.536138Z",
          "iopub.status.idle": "2025-12-10T10:55:16.833268Z",
          "shell.execute_reply": "2025-12-10T10:55:16.832616Z",
          "shell.execute_reply.started": "2025-12-10T10:55:16.536352Z"
        },
        "id": "b4b077fc",
        "outputId": "37d6e0e8-1e82-41af-c972-1a54fd159ffc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# AlphaZeroTrainer -  Evaluation & Training methods\n",
        "import csv\n",
        "\n",
        "# Preload SFT teacher model once\n",
        "SFT_TEACHER = None\n",
        "SFT_PATH = SFT_MODEL_PATH\n",
        "if os.path.exists(SFT_PATH):\n",
        "    SFT_TEACHER = SmallResNet(num_res_blocks=6, num_channels=64, action_size=4672).to(device)\n",
        "    checkpoint = torch.load(SFT_PATH, map_location=device)\n",
        "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "        SFT_TEACHER.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        SFT_TEACHER.load_state_dict(checkpoint)\n",
        "    SFT_TEACHER.eval()\n",
        "    print(\"üë®‚Äçüè´ SFT teacher loaded\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è No SFT teacher found; Random-only opponent available\")\n",
        "\n",
        "\n",
        "def evaluate_vs_random(self, num_games=100):\n",
        "    wins, losses, draws = 0, 0, 0\n",
        "    self.model.eval()\n",
        "\n",
        "    for _ in tqdm(range(num_games), desc=\"Evaluating\"):\n",
        "        board = chess.Board()\n",
        "        ai_plays_white = random.choice([True, False])\n",
        "        move_count = 0\n",
        "        prev_board = None\n",
        "\n",
        "        while not board.is_game_over() and move_count < 500:\n",
        "            if (board.turn == chess.WHITE) == ai_plays_white:\n",
        "                policy = self.mcts.search(board, num_simulations=50, temperature=0.0, prev_board=prev_board, root_noise=False)\n",
        "                legal_moves = list(board.legal_moves)\n",
        "                legal_probs = [policy[self.converter.encode_move(m.uci())] if self.converter.encode_move(m.uci()) is not None else 0.0 for m in legal_moves]\n",
        "\n",
        "                if sum(legal_probs) > 0:\n",
        "                    # Greedy for evaluation, but keep safety\n",
        "                    probs = np.array(legal_probs, dtype=np.float64)\n",
        "                    probs = np.nan_to_num(probs, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                    probs = np.clip(probs, 0.0, None)\n",
        "                    chosen = legal_moves[np.argmax(probs)]\n",
        "                else:\n",
        "                    chosen = random.choice(legal_moves)\n",
        "                prev_for_next = board.copy()\n",
        "                board.push(chosen)\n",
        "                prev_board = prev_for_next\n",
        "            else:\n",
        "                prev_for_next = board.copy()\n",
        "                board.push(random.choice(list(board.legal_moves)))\n",
        "                prev_board = prev_for_next\n",
        "            move_count += 1\n",
        "\n",
        "        result = board.result()\n",
        "\n",
        "        if result == \"1/2-1/2\":\n",
        "            outcome = 0.0\n",
        "        elif result == \"1-0\":\n",
        "            outcome = 1.0 if ai_plays_white else -1.0\n",
        "        else:\n",
        "            outcome = -1.0 if ai_plays_white else 1.0\n",
        "\n",
        "        if outcome > 0: wins += 1\n",
        "        elif outcome < 0: losses += 1\n",
        "        else: draws += 1\n",
        "\n",
        "    winrate = (wins / num_games * 100) if num_games > 0 else 0\n",
        "    return wins, losses, draws, winrate\n",
        "\n",
        "def evaluate_vs_sft(self, sft_path=None, num_games=10):\n",
        "    # 1. Load SFT Model\n",
        "    if sft_path is None:\n",
        "        sft_path = SFT_MODEL_PATH\n",
        "\n",
        "    try:\n",
        "        sft_opponent = SmallResNet(num_res_blocks=6, num_channels=64, action_size=4672).to(self.device)\n",
        "        checkpoint = torch.load(sft_path, map_location=self.device)\n",
        "\n",
        "        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "            sft_opponent.load_state_dict(checkpoint['model_state_dict'])\n",
        "        else:\n",
        "            sft_opponent.load_state_dict(checkpoint)\n",
        "\n",
        "        sft_opponent.eval() # Freeze SFT\n",
        "        print(\"‚úÖ SFT Opponent Loaded Successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading SFT model: {e}\")\n",
        "        print(\"üí° H√£y ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n file .pth\")\n",
        "        return\n",
        "\n",
        "    wins, losses, draws = 0, 0, 0\n",
        "    self.model.eval()\n",
        "\n",
        "    print(f\"\\n‚öîÔ∏è  RL BOT (MCTS) vs SFT MODEL (Raw Policy) | {num_games} Games\")\n",
        "\n",
        "    for i in range(num_games):\n",
        "        board = chess.Board()\n",
        "        ai_plays_white = random.choice([True, False])\n",
        "\n",
        "        # In th√¥ng tin v√°n ƒë·∫•u\n",
        "        color_str = \"White\" if ai_plays_white else \"Black\"\n",
        "        print(f\"   Game {i+1}: RL Bot ({color_str}) vs SFT...\", end=\" \", flush=True)\n",
        "\n",
        "        move_count = 0\n",
        "        prev_board = None\n",
        "\n",
        "        while not board.is_game_over() and move_count < 200:\n",
        "            is_rl_turn = (board.turn == chess.WHITE) == ai_plays_white\n",
        "\n",
        "            if is_rl_turn:\n",
        "                # === RL BOT ===\n",
        "                policy = self.mcts.search(board, num_simulations=50, temperature=0.0, prev_board=prev_board, root_noise=False)\n",
        "                legal_moves = list(board.legal_moves)\n",
        "                legal_probs = []\n",
        "                for m in legal_moves:\n",
        "                    idx = self.converter.encode_move(m.uci())\n",
        "                    if idx is not None:\n",
        "                        legal_probs.append(policy[idx])\n",
        "                    else:\n",
        "                        legal_probs.append(0.0)\n",
        "\n",
        "                if sum(legal_probs) > 0:\n",
        "                    chosen = legal_moves[np.argmax(legal_probs)]\n",
        "                else:\n",
        "                    chosen = random.choice(legal_moves)\n",
        "            else:\n",
        "                # === SFT OPPONENT ===\n",
        "                state = self.converter.board_to_tensor(board, prev_board=prev_board).reshape(1, 32, 8, 8)\n",
        "                state_t = torch.FloatTensor(state).to(self.device)\n",
        "\n",
        "                # 2. Forward pass\n",
        "                with torch.no_grad():\n",
        "                    policy_logits, _ = sft_opponent(state_t)\n",
        "\n",
        "                # 3. L·∫•y probabilities\n",
        "                policy_probs = torch.softmax(policy_logits, dim=1).cpu().numpy()[0]\n",
        "\n",
        "                # 4. Masking legal moves\n",
        "                legal_moves = list(board.legal_moves)\n",
        "                move_probs = []\n",
        "                for m in legal_moves:\n",
        "                    idx = self.converter.encode_move(m.uci())\n",
        "                    if idx is not None:\n",
        "                        move_probs.append(policy_probs[idx])\n",
        "                    else:\n",
        "                        move_probs.append(0.0)\n",
        "\n",
        "                # 5. Greedy selection\n",
        "                if sum(move_probs) > 0:\n",
        "                    chosen = legal_moves[np.argmax(move_probs)]\n",
        "                else:\n",
        "                    chosen = random.choice(legal_moves) # Fallback\n",
        "\n",
        "            prev_for_next = board.copy()\n",
        "            board.push(chosen)\n",
        "            prev_board = prev_for_next\n",
        "            move_count += 1\n",
        "\n",
        "        result = board.result()\n",
        "        if result == \"1/2-1/2\":\n",
        "            draws += 1\n",
        "            print(\"Draw ü§ù\")\n",
        "        elif result == \"1-0\":\n",
        "            if ai_plays_white: wins += 1; print(\"Win üèÜ\")\n",
        "            else: losses += 1; print(\"Loss ‚ùå\")\n",
        "        else: # 0-1\n",
        "            if ai_plays_white: losses += 1; print(\"Loss ‚ùå\")\n",
        "            else: wins += 1; print(\"Win üèÜ\")\n",
        "\n",
        "    winrate = (wins / num_games * 100) if num_games > 0 else 0\n",
        "    print(f\"\\nüìä TOTAL: Wins={wins}, Losses={losses}, Draws={draws} | Winrate: {winrate:.1f}%\")\n",
        "    return wins, losses, draws, winrate\n",
        "\n",
        "\n",
        "def _opponent_move(self, board, opponent_model, prev_board_ref):\n",
        "    if opponent_model is None:\n",
        "        # Random opponent\n",
        "        prev_for_next = board.copy()\n",
        "        board.push(random.choice(list(board.legal_moves)))\n",
        "        prev_board_ref[0] = prev_for_next\n",
        "        return\n",
        "    # SFT teacher move (greedy on policy)\n",
        "    state = self.converter.board_to_tensor(board, prev_board=prev_board_ref[0]).reshape(1, 32, 8, 8)\n",
        "    state_t = torch.FloatTensor(state).to(self.device)\n",
        "    with torch.no_grad():\n",
        "        policy_logits, _ = opponent_model(state_t)\n",
        "\n",
        "    policy = torch.softmax(policy_logits, dim=1).cpu().numpy()[0]\n",
        "    legal_moves = list(board.legal_moves)\n",
        "    legal_probs = [policy[self.converter.encode_move(m.uci())] if self.converter.encode_move(m.uci()) is not None else 0.0 for m in legal_moves]\n",
        "    if sum(legal_probs) > 0:\n",
        "        probs = np.array(legal_probs, dtype=np.float64)\n",
        "        probs = np.nan_to_num(probs, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        probs = np.clip(probs, 0.0, None)\n",
        "        chosen = legal_moves[np.argmax(probs)]\n",
        "    else:\n",
        "        chosen = random.choice(legal_moves)\n",
        "    prev_for_next = board.copy()\n",
        "    board.push(chosen)\n",
        "    prev_board_ref[0] = prev_for_next\n",
        "\n",
        "\n",
        "def train_iteration(self, num_games=50, batch_size=64, train_steps=500, eval_vs_random=False, iteration=0):\n",
        "    # Ch·ªâ self-play khi training\n",
        "    num_sims = 100  \n",
        "    print(f\"\\n{'='*60}\\nSelf-Play: {num_games} games (MCTS {num_sims} sims)\\n{'='*60}\")\n",
        "    self.model.eval()\n",
        "    for i in tqdm(range(num_games), desc=\"Self-Play\"):\n",
        "        samples, _, _ = self.self_play_game(num_simulations=num_sims, temperature=1.0)\n",
        "        self.replay_buffer.add_batch(samples)\n",
        "\n",
        "\n",
        "    print(f\"üìä Buffer: {len(self.replay_buffer)} samples\")\n",
        "\n",
        "    if not self.use_self_play:\n",
        "        if len(self.replay_buffer) > 5000:\n",
        "            print(\"üöÄ K√çCH HO·∫†T CH·∫æ ƒê·ªò SELF-PLAY: Bot ƒë√£ ƒë·ªß m·∫°nh!\")\n",
        "            self.use_self_play = True\n",
        "\n",
        "    print(f\"\\n{'='*60}\\nTraining\\n{'='*60}\")\n",
        "    epochs = 5\n",
        "    batches_per_epoch = max(1, len(self.replay_buffer) // batch_size)\n",
        "    total_steps = batches_per_epoch * epochs\n",
        "    print(f\"Training: {epochs} epochs, {batches_per_epoch} batches/epoch = {total_steps} total steps\")\n",
        "\n",
        "    for step in range(total_steps):\n",
        "        batch = self.replay_buffer.sample(batch_size)\n",
        "        loss, p_loss, v_loss = self.train_on_batch(batch)\n",
        "        if step % 100 == 0:\n",
        "            epoch = step // batches_per_epoch + 1\n",
        "            print(f\"  Epoch {epoch}/{epochs}, Step {step}/{total_steps}: Loss={loss:.4f}, P={p_loss:.4f}, V={v_loss:.4f}\")\n",
        "\n",
        "    # Step LR scheduler after each iteration\n",
        "    self.scheduler.step()\n",
        "\n",
        "    if eval_vs_random:\n",
        "        wins, losses, draws, winrate = self.evaluate_vs_random(num_games=10)\n",
        "        print(f\"üéØ Post-Train Eval Random: {winrate:.1f}%\")\n",
        "\n",
        "        wins, losses, draws, winrate = self.evaluate_vs_sft(sft_path='/content/drive/MyDrive/Chess/models/model_best_4672.pth', num_games=10)\n",
        "        print(f\"üéØ Post-Train Eval SFT: {winrate:.1f}%\")\n",
        "        return wins, losses, draws, winrate, len(self.replay_buffer), num_sims\n",
        "    else:\n",
        "        return None, None, None, None, len(self.replay_buffer), num_sims\n",
        "\n",
        "\n",
        "def train(self, num_iterations=100, games_per_iter=40, save_interval=5, target_winrate=90.0, log_csv_path=None):\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\nüöÄ AlphaZero RL Training\\n\" + \"=\"*60)\n",
        "    best_winrate = 0.0\n",
        "\n",
        "    # Setup CSV logging\n",
        "    if log_csv_path is None:\n",
        "        log_csv_path = TRAINING_LOG_PATH\n",
        "    os.makedirs(os.path.dirname(log_csv_path), exist_ok=True)\n",
        "    if not os.path.exists(log_csv_path):\n",
        "        with open(log_csv_path, 'w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"iteration\", \"wins\", \"losses\", \"draws\", \"winrate\", \"buffer_size\", \"num_sims\", \"lr\"])\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        print(f\"\\n{'='*60}\\nIteration {iteration + 1}/{num_iterations}\\n{'='*60}\")\n",
        "        eval_vs_random = (iteration + 1) % 2 == 0\n",
        "        current_lr = self.optimizer.param_groups[0]['lr']\n",
        "        wins, losses, draws, winrate, buffer_size, num_sims = self.train_iteration(num_games=games_per_iter, eval_vs_random=eval_vs_random, iteration=iteration)\n",
        "\n",
        "        # Track and save best\n",
        "        if winrate and winrate > best_winrate:\n",
        "            best_winrate = winrate\n",
        "            self.save_checkpoint(MODEL_BEST_PATH)\n",
        "            print(f\"üèÜ NEW BEST: {winrate:.1f}%\")\n",
        "\n",
        "        # Write CSV row if evaluated\n",
        "        if winrate is not None:\n",
        "            with open(log_csv_path, 'a', newline='') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([iteration + 1, wins, losses, draws, winrate, buffer_size, num_sims, current_lr])\n",
        "\n",
        "        # Always save latest (overwrite)\n",
        "        self.save_checkpoint(MODEL_LATEST_PATH)\n",
        "        self.replay_buffer.save(BUFFER_LATEST_PATH)\n",
        "        # ‚úÖ Also persist move_map after each iteration\n",
        "\n",
        "        # Early stopping on target winrate\n",
        "        if winrate and winrate >= target_winrate:\n",
        "            print(f\"\\n{'='*60}\\nüéâ EARLY STOP: Target {target_winrate:.1f}% reached (winrate {winrate:.1f}%)\\n{'='*60}\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\n‚úÖ Training completed! Best winrate: {best_winrate:.1f}%\")\n",
        "\n",
        "AlphaZeroTrainer.evaluate_vs_sft = evaluate_vs_sft\n",
        "AlphaZeroTrainer.evaluate_vs_random = evaluate_vs_random\n",
        "AlphaZeroTrainer.train_iteration = train_iteration\n",
        "AlphaZeroTrainer.train = train\n",
        "AlphaZeroTrainer._opponent_move = _opponent_move"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e0896e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-12-10T10:55:16.835414Z",
          "iopub.status.busy": "2025-12-10T10:55:16.835205Z",
          "iopub.status.idle": "2025-12-10T10:55:18.032371Z",
          "shell.execute_reply": "2025-12-10T10:55:18.031676Z",
          "shell.execute_reply.started": "2025-12-10T10:55:16.835398Z"
        },
        "id": "c1e0896e",
        "outputId": "912cdb68-6f58-4407-94a1-4ef1e44ba4f8",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Loading Model: /content/drive/MyDrive/models/model_rl_best.pth\n",
            "üìñ Loaded: /content/drive/MyDrive/models/model_rl_best.pth\n",
            "‚úÖ Model loaded!\n",
            "‚ùå Buffer not found: /content/drive/MyDrive/Chess/models/buffer_latest.pkl\n"
          ]
        }
      ],
      "source": [
        "# Initialize\n",
        "model = SmallResNet(num_res_blocks=6, num_channels=64, action_size=4672)\n",
        "trainer = AlphaZeroTrainer(model=model, device=device, lr=0.00005, weight_decay=1e-4)\n",
        "\n",
        "# Use hard-coded canonical map (no PKL load/freeze)\n",
        "trainer.converter = ChessConverter(move_to_idx=CANONICAL_MOVE_TO_IDX, idx_to_move=CANONICAL_IDX_TO_MOVE)\n",
        "trainer.mcts.converter = ChessConverter(move_to_idx=CANONICAL_MOVE_TO_IDX, idx_to_move=CANONICAL_IDX_TO_MOVE)\n",
        "\n",
        "\n",
        "MODEL_LOAD_PATH = os.path.join(MODEL_PATH, 'model_rl_best.pth')\n",
        "\n",
        "# # Load Model\n",
        "if os.path.exists(MODEL_LOAD_PATH):\n",
        "    print(f\"üîÑ Loading Model: {MODEL_LOAD_PATH}\")\n",
        "    trainer.load_checkpoint(MODEL_LOAD_PATH)\n",
        "    print(\"‚úÖ Model loaded!\")\n",
        "else:\n",
        "    print(f\"‚ùå Model not found: {MODEL_LOAD_PATH}\")\n",
        "    SFT_PATH = SFT_MODEL_PATH\n",
        "    if os.path.exists(SFT_PATH):\n",
        "        trainer.load_checkpoint(SFT_PATH)\n",
        "        print(\"Load SFT model as starting point.\")\n",
        "    else:\n",
        "        print(\"No SFT model found; training from scratch.\")\n",
        "\n",
        "# # Load Buffer\n",
        "if os.path.exists('/content/drive/MyDrive/Chess/models/buffer_latest.pkl'):\n",
        "    trainer.replay_buffer.load('/content/drive/MyDrive/Chess/models/buffer_latest.pkl')\n",
        "else:\n",
        "    print(f\"‚ùå Buffer not found: {'/content/drive/MyDrive/Chess/models/buffer_latest.pkl'}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8607eeeb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-10T10:56:51.842659Z",
          "iopub.status.busy": "2025-12-10T10:56:51.842169Z",
          "iopub.status.idle": "2025-12-10T12:19:34.128349Z",
          "shell.execute_reply": "2025-12-10T12:19:34.127389Z",
          "shell.execute_reply.started": "2025-12-10T10:56:51.842637Z"
        },
        "id": "8607eeeb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# # Start Training\n",
        "# trainer.train(num_iterations=20, games_per_iter=30, save_interval=5, target_winrate=100.0, log_csv_path=os.path.join(SAVE_PATH, \"training_log.csv\"))\n",
        "# print(\"\\n Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b61cddeb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b61cddeb",
        "outputId": "0e5a20bd-4f12-45f7-d4ff-500deea31c21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ ƒêANG ƒê√ÅNH GI√Å VS RANDOM (20 v√°n)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Playing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [04:22<00:00, 13.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä K·∫æT QU·∫¢ T·ªîNG H·ª¢P:\n",
            "   üèÜ Th·∫Øng: 16 | ‚ùå Thua: 0 | ü§ù H√≤a: 4\n",
            "   üìà T·ªâ l·ªá th·∫Øng: 80.0%\n",
            "   üîç Chi ti·∫øt l√Ω do: {'checkmate': 16, 'stalemate': 0, 'insufficient': 0, 'repetition': 4, 'timeout': 0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(16, 0, 4, 80.0)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Evaluate vs Random (Detailed)\n",
        "import random\n",
        "import chess\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_vs_random_final(self, num_games=20):\n",
        "    print(f\"\\nüß™ ƒêANG ƒê√ÅNH GI√Å VS RANDOM ({num_games} v√°n)...\")\n",
        "    wins, losses, draws = 0, 0, 0\n",
        "    \n",
        "    reasons = {\"checkmate\": 0, \"stalemate\": 0, \"insufficient\": 0, \"repetition\": 0, \"timeout\": 0}\n",
        "\n",
        "    self.model.eval() \n",
        "\n",
        "    for i in tqdm(range(num_games), desc=\"Playing\"):\n",
        "        board = chess.Board()\n",
        "        ai_plays_white = random.choice([True, False])\n",
        "        prev_board = None\n",
        "        move_count = 0\n",
        "\n",
        "        while not board.is_game_over() and move_count < 500:\n",
        "            if (board.turn == chess.WHITE) == ai_plays_white:\n",
        "                \n",
        "                policy = self.mcts.search(board, num_simulations=50, temperature=0.0, prev_board=prev_board, root_noise=False)\n",
        "                legal_moves = list(board.legal_moves)\n",
        "                legal_probs = [policy[self.converter.encode_move(m.uci())] if self.converter.encode_move(m.uci()) is not None else 0.0 for m in legal_moves]\n",
        "\n",
        "                if sum(legal_probs) > 0:\n",
        "                    # Greedy for evaluation, but keep safety\n",
        "                    probs = np.array(legal_probs, dtype=np.float64)\n",
        "                    probs = np.nan_to_num(probs, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                    probs = np.clip(probs, 0.0, None)\n",
        "                    chosen = legal_moves[np.argmax(probs)]\n",
        "                else:\n",
        "                    chosen = random.choice(legal_moves)\n",
        "                prev_for_next = board.copy()\n",
        "                board.push(chosen)\n",
        "                prev_board = prev_for_next\n",
        "            else:\n",
        "                prev_for_next = board.copy()\n",
        "                board.push(random.choice(list(board.legal_moves)))\n",
        "                prev_board = prev_for_next\n",
        "            move_count += 1\n",
        "\n",
        "        result = board.result()\n",
        "\n",
        "        \n",
        "        if board.is_checkmate():\n",
        "            reasons[\"checkmate\"] += 1\n",
        "        elif board.is_stalemate():\n",
        "            reasons[\"stalemate\"] += 1\n",
        "        elif board.is_insufficient_material():\n",
        "            reasons[\"insufficient\"] += 1\n",
        "        elif board.is_repetition():\n",
        "            reasons[\"repetition\"] += 1\n",
        "        elif move_count >= 500:\n",
        "            reasons[\"timeout\"] += 1\n",
        "\n",
        "        # T√≠nh ƒëi·ªÉm\n",
        "        if result == \"1-0\":\n",
        "            if ai_plays_white: wins += 1\n",
        "            else: losses += 1\n",
        "        elif result == \"0-1\":\n",
        "            if ai_plays_white: losses += 1\n",
        "            else: wins += 1\n",
        "        else:\n",
        "            draws += 1\n",
        "\n",
        "    winrate = (wins / num_games) * 100\n",
        "    print(f\"\\nüìä K·∫æT QU·∫¢ T·ªîNG H·ª¢P:\")\n",
        "    print(f\"   üèÜ Th·∫Øng: {wins} | ‚ùå Thua: {losses} | ü§ù H√≤a: {draws}\")\n",
        "    print(f\"   üìà T·ªâ l·ªá th·∫Øng: {winrate:.1f}%\")\n",
        "    print(f\"   üîç Chi ti·∫øt l√Ω do: {reasons}\")\n",
        "\n",
        "    if reasons['stalemate'] > 0:\n",
        "        print(\"   ‚ö†Ô∏è  C·∫¢NH B√ÅO: C√≥ tr·∫≠n h√≤a PAT (Stalemate). Bot d·ªìn vua nh∆∞ng qu√™n chi·∫øu h·∫øt!\")\n",
        "\n",
        "    return wins, losses, draws, winrate\n",
        "\n",
        "AlphaZeroTrainer.evaluate_vs_random_final = evaluate_vs_random_final\n",
        "\n",
        "# Ch·∫°y th·ª≠ 20 v√°n\n",
        "trainer.evaluate_vs_random_final(num_games=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d00aa60",
      "metadata": {
        "id": "6d00aa60"
      },
      "outputs": [],
      "source": [
        "# Human vs Bot Play\n",
        "import time\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "def play_human_vs_bot(trainer, num_simulations=50):\n",
        "    board = chess.Board()\n",
        "\n",
        "    while True:\n",
        "        choice = input(\"B·∫°n mu·ªën c·∫ßm qu√¢n n√†o? (w=Tr·∫Øng, b=ƒêen): \").lower()\n",
        "        if choice in ['w', 'b']:\n",
        "            human_is_white = (choice == 'w')\n",
        "            break\n",
        "\n",
        "    print(f\"üéÆ B·∫Øt ƒë·∫ßu! B·∫°n c·∫ßm {'TR·∫ÆNG' if human_is_white else 'ƒêEN'}\")\n",
        "    time.sleep(1)\n",
        "\n",
        "    while not board.is_game_over():\n",
        "        clear_output(wait=True)\n",
        "        display(board)\n",
        "\n",
        "        is_human_turn = (board.turn == chess.WHITE) == human_is_white\n",
        "\n",
        "        if is_human_turn:\n",
        "            while True:\n",
        "                try:\n",
        "                    move_str = input(f\"L∆∞·ª£t b·∫°n ({'Tr·∫Øng' if board.turn == chess.WHITE else 'ƒêen'}): \")\n",
        "                    # H·ªó tr·ª£ c·∫£ SAN (e4) v√† UCI (e2e4)\n",
        "                    try:\n",
        "                        move = board.parse_san(move_str)\n",
        "                    except:\n",
        "                        move = board.parse_uci(move_str)\n",
        "\n",
        "                    if move in board.legal_moves:\n",
        "                        board.push(move)\n",
        "                        break\n",
        "                    else:\n",
        "                        print(\"‚ùå N∆∞·ªõc ƒëi kh√¥ng h·ª£p l·ªá. Th·ª≠ l·∫°i.\")\n",
        "                except ValueError:\n",
        "                    print(\"‚ö†Ô∏è L·ªói c√∫ ph√°p. H√£y nh·∫≠p l·∫°i (VD: e4, Nf3, e2e4...)\")\n",
        "        else:\n",
        "            print(\"ü§ñ Bot ƒëang suy nghƒ©...\")\n",
        "            # MCTS Search\n",
        "            policy = trainer.mcts.search(board, num_simulations=num_simulations, temperature=0.0, root_noise=False)\n",
        "\n",
        "            # Ch·ªçn move t·ª´ policy\n",
        "            legal_moves = list(board.legal_moves)\n",
        "            legal_probs = []\n",
        "            for m in legal_moves:\n",
        "                idx = trainer.converter.encode_move(m.uci())\n",
        "                if idx is not None:\n",
        "                    legal_probs.append(policy[idx])\n",
        "                else:\n",
        "                    legal_probs.append(0.0)\n",
        "\n",
        "            if sum(legal_probs) > 0:\n",
        "                chosen = legal_moves[np.argmax(legal_probs)]\n",
        "            else:\n",
        "                chosen = random.choice(legal_moves)\n",
        "\n",
        "            board.push(chosen)\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    display(board)\n",
        "    print(\"üèÅ TR·∫¨N ƒê·∫§U K·∫æT TH√öC!\")\n",
        "    print(f\"K·∫øt qu·∫£: {board.result()}\")\n",
        "\n",
        "# Ch·∫°y game\n",
        "play_human_vs_bot(trainer, num_simulations=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa312eec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "aa312eec",
        "outputId": "04d5697c-5cdb-434f-b149-68006270805b"
      },
      "outputs": [
        {
          "data": {
            "image/svg+xml": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" viewBox=\"0 0 390 390\" width=\"390\" height=\"390\"><desc><pre>. . . . . . . .\n",
              ". . . . . Q . .\n",
              ". . B . . . . k\n",
              ". . . . . . . .\n",
              "P . . . . B . P\n",
              ". . . P . . . .\n",
              ". . . P . . P .\n",
              "R N . . K . . .</pre></desc><defs><g id=\"white-pawn\" class=\"white pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"white-knight\" class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#000000; stroke:#000000;\" /></g><g id=\"white-bishop\" class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-queen\" class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g><radialGradient id=\"check_gradient\" r=\"0.5\"><stop offset=\"0%\" stop-color=\"#ff0000\" stop-opacity=\"1.0\" /><stop offset=\"50%\" stop-color=\"#e70000\" stop-opacity=\"1.0\" /><stop offset=\"100%\" stop-color=\"#9e0000\" stop-opacity=\"0.0\" /></radialGradient></defs><rect x=\"7.5\" y=\"7.5\" width=\"375\" height=\"375\" fill=\"none\" stroke=\"#212121\" stroke-width=\"15\" /><g transform=\"translate(20, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 1) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light b1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark lastmove f4\" stroke=\"none\" fill=\"#aaa23b\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark lastmove e5\" stroke=\"none\" fill=\"#aaa23b\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"check\" fill=\"url(#check_gradient)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(15, 330)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(60, 330)\" /><use href=\"#white-king\" xlink:href=\"#white-king\" transform=\"translate(195, 330)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(150, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(285, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(150, 240)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(15, 195)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(240, 195)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(330, 195)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(105, 105)\" /><use href=\"#black-king\" xlink:href=\"#black-king\" transform=\"translate(330, 105)\" /><use href=\"#white-queen\" xlink:href=\"#white-queen\" transform=\"translate(240, 60)\" /></svg>"
            ],
            "text/plain": [
              "Board('8/5Q2/2B4k/8/P4B1P/3P4/3P2P1/RN2K3 b - - 15 39')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèÅ K·∫æT QU·∫¢: 1-0\n",
            "Tr·∫Øng th·∫Øng!\n"
          ]
        }
      ],
      "source": [
        "#BOT RL VS MINIMAX\n",
        "from IPython.display import display, clear_output\n",
        "import chess\n",
        "import random\n",
        "import time\n",
        "\n",
        "PIECE_VALUES = {\n",
        "    chess.PAWN: 100, chess.KNIGHT: 320, chess.BISHOP: 330,\n",
        "    chess.ROOK: 500, chess.QUEEN: 900, chess.KING: 20000\n",
        "}\n",
        "\n",
        "def evaluate_board_minimax(board):\n",
        "    if board.is_checkmate():\n",
        "        if board.turn == chess.WHITE: return -999999\n",
        "        else: return 999999\n",
        "    if board.is_stalemate() or board.is_insufficient_material(): return 0\n",
        "\n",
        "    score = 0\n",
        "    for square in chess.SQUARES:\n",
        "        piece = board.piece_at(square)\n",
        "        if piece:\n",
        "            val = PIECE_VALUES.get(piece.piece_type, 0)\n",
        "            if piece.color == chess.WHITE: score += val\n",
        "            else: score -= val\n",
        "    return score\n",
        "\n",
        "def minimax_alpha_beta(board, depth, alpha, beta, maximizing_player):\n",
        "    if depth == 0 or board.is_game_over():\n",
        "        return evaluate_board_minimax(board)\n",
        "\n",
        "    legal_moves = list(board.legal_moves)\n",
        "    legal_moves.sort(key=lambda move: board.is_capture(move), reverse=True)\n",
        "\n",
        "    if maximizing_player:\n",
        "        max_eval = -float('inf')\n",
        "        for move in legal_moves:\n",
        "            board.push(move)\n",
        "            eval = minimax_alpha_beta(board, depth - 1, alpha, beta, False)\n",
        "            board.pop()\n",
        "            max_eval = max(max_eval, eval)\n",
        "            alpha = max(alpha, eval)\n",
        "            if beta <= alpha: break\n",
        "        return max_eval\n",
        "    else:\n",
        "        min_eval = float('inf')\n",
        "        for move in legal_moves:\n",
        "            board.push(move)\n",
        "            eval = minimax_alpha_beta(board, depth - 1, alpha, beta, True)\n",
        "            board.pop()\n",
        "            min_eval = min(min_eval, eval)\n",
        "            beta = min(beta, eval)\n",
        "            if beta <= alpha: break\n",
        "        return min_eval\n",
        "\n",
        "def get_best_move_minimax(board, depth=2):\n",
        "    best_move = None\n",
        "    max_eval = -float('inf')\n",
        "    min_eval = float('inf')\n",
        "    alpha = -float('inf')\n",
        "    beta = float('inf')\n",
        "\n",
        "    legal_moves = list(board.legal_moves)\n",
        "    random.shuffle(legal_moves) \n",
        "    legal_moves.sort(key=lambda move: board.is_capture(move), reverse=True)\n",
        "\n",
        "    is_maximizing = (board.turn == chess.WHITE)\n",
        "\n",
        "    for move in legal_moves:\n",
        "        board.push(move)\n",
        "        val = minimax_alpha_beta(board, depth - 1, alpha, beta, not is_maximizing)\n",
        "        board.pop()\n",
        "\n",
        "        if is_maximizing:\n",
        "            if val > max_eval:\n",
        "                max_eval = val\n",
        "                best_move = move\n",
        "            alpha = max(alpha, val)\n",
        "        else:\n",
        "            if val < min_eval:\n",
        "                min_eval = val\n",
        "                best_move = move\n",
        "            beta = min(beta, val)\n",
        "\n",
        "    return best_move if best_move else random.choice(legal_moves)\n",
        "\n",
        "def watch_bot_vs_minimax(trainer, minimax_depth=2, delay=0.5):\n",
        "    board = chess.Board()\n",
        "    rl_plays_white = random.choice([True, False])\n",
        "\n",
        "    print(f\"‚öîÔ∏è RL Bot ({'Tr·∫Øng' if rl_plays_white else 'ƒêen'}) vs Minimax ({'ƒêen' if rl_plays_white else 'Tr·∫Øng'})\")\n",
        "    time.sleep(2)\n",
        "\n",
        "    move_count = 0\n",
        "    prev_board = None\n",
        "\n",
        "    while not board.is_game_over() and move_count < 200:\n",
        "        clear_output(wait=True)\n",
        "        display(board)\n",
        "\n",
        "        turn_name = \"Tr·∫Øng\" if board.turn == chess.WHITE else \"ƒêen\"\n",
        "        is_rl_turn = (board.turn == chess.WHITE) == rl_plays_white\n",
        "\n",
        "        print(f\"N∆∞·ªõc {move_count+1}: {turn_name} ƒëang ƒëi...\")\n",
        "\n",
        "        if is_rl_turn:\n",
        "            # === RL BOT ===\n",
        "            policy = trainer.mcts.search(board, num_simulations=50, temperature=1.0, prev_board=prev_board, root_noise=False)\n",
        "            legal_moves = list(board.legal_moves)\n",
        "            legal_probs = [policy[trainer.converter.encode_move(m.uci())] if trainer.converter.encode_move(m.uci()) is not None else 0.0 for m in legal_moves]\n",
        "\n",
        "            if sum(legal_probs) > 0:\n",
        "                chosen = legal_moves[np.argmax(legal_probs)]\n",
        "            else:\n",
        "                chosen = random.choice(legal_moves)\n",
        "\n",
        "            print(f\"ü§ñ RL Bot ch·ªçn: {board.san(chosen)}\")\n",
        "            prev_for_next = board.copy()\n",
        "            board.push(chosen)\n",
        "            prev_board = prev_for_next\n",
        "        else:\n",
        "            # === MINIMAX ===\n",
        "            move = get_best_move_minimax(board, depth=minimax_depth)\n",
        "\n",
        "            print(f\"‚ôüÔ∏è Minimax ch·ªçn: {board.san(move)}\")\n",
        "            prev_for_next = board.copy()\n",
        "            board.push(move)\n",
        "            prev_board = prev_for_next\n",
        "\n",
        "        move_count += 1\n",
        "        time.sleep(delay) # Delay for viewing\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    display(board)\n",
        "    print(f\"üèÅ K·∫æT QU·∫¢: {board.result()}\")\n",
        "    if board.is_game_over():\n",
        "        if board.result() == \"1-0\": print(\"Tr·∫Øng th·∫Øng!\")\n",
        "        elif board.result() == \"0-1\": print(\"ƒêen th·∫Øng!\")\n",
        "        else: print(\"H√≤a!\")\n",
        "\n",
        "# Demo depth 2\n",
        "watch_bot_vs_minimax(trainer, minimax_depth=2, delay=0.3)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 8967718,
          "sourceId": 14085211,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
