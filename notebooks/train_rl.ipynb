{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RL Training - Reinforcement Learning with MCTS Self-Play\n",
                "\n",
                "**Self-contained notebook** - All classes defined inline!\n",
                "\n",
                "**Features:**\n",
                "- ‚úÖ Loads SFT checkpoint automatically\n",
                "- ‚úÖ MCTS-guided self-play\n",
                "- ‚úÖ Complete implementation in single notebook\n",
                "- ‚úÖ Google Colab ready"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch numpy python-chess tqdm -q\n",
                "\n",
                "# Mount Google Drive\n",
                "try:\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "except:\n",
                "    print(\"Not in Colab\")\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "import numpy as np\n",
                "import chess\n",
                "import os\n",
                "import random\n",
                "import pickle\n",
                "from collections import deque\n",
                "from tqdm import tqdm\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\" Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define All Classes (Copy from SFT + Add MCTS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CANONICAL MOVE ENCODER\n",
                "# ============================================================\n",
                "class CanonicalMoveEncoder:\n",
                "    def __init__(self):\n",
                "        self.move_to_idx, self.idx_to_move = self._build_canonical_map()\n",
                "    \n",
                "    def _build_canonical_map(self):\n",
                "        move_to_idx, idx_to_move = {}, {}\n",
                "        idx = 0\n",
                "        \n",
                "        # Queen moves\n",
                "        directions = [(1,0),(1,1),(0,1),(-1,1),(-1,0),(-1,-1),(0,-1),(1,-1)]\n",
                "        for from_sq in range(64):\n",
                "            for d_r, d_f in directions:\n",
                "                for dist in range(1, 8):\n",
                "                    to_rank = (from_sq // 8) + d_r * dist\n",
                "                    to_file = (from_sq % 8) + d_f * dist\n",
                "                    if 0 <= to_rank < 8 and 0 <= to_file < 8:\n",
                "                        dest = to_rank * 8 + to_file\n",
                "                        uci = chess.Move(from_sq, dest).uci()\n",
                "                        move_to_idx[uci] = idx\n",
                "                        idx_to_move[idx] = uci\n",
                "                    idx += 1\n",
                "        \n",
                "        # Knight moves\n",
                "        knight_moves = [(2,1),(1,2),(-1,2),(-2,1),(-2,-1),(-1,-2),(1,-2),(2,-1)]\n",
                "        for from_sq in range(64):\n",
                "            for d_r, d_f in knight_moves:\n",
                "                to_rank = (from_sq // 8) + d_r\n",
                "                to_file = (from_sq % 8) + d_f\n",
                "                if 0 <= to_rank < 8 and 0 <= to_file < 8:\n",
                "                    dest = to_rank * 8 + to_file\n",
                "                    uci = chess.Move(from_sq, dest).uci()\n",
                "                    move_to_idx[uci] = idx\n",
                "                    idx_to_move[idx] = uci\n",
                "                idx += 1\n",
                "        \n",
                "        # Underpromotions\n",
                "        for from_sq in range(64):\n",
                "            rank = from_sq // 8\n",
                "            file = from_sq % 8\n",
                "            rank_step = 1 if rank == 6 else (-1 if rank == 1 else 0)\n",
                "            for f_step in [0, -1, 1]:\n",
                "                for p in ['r', 'b', 'n']:\n",
                "                    if rank_step != 0:\n",
                "                        to_rank = rank + rank_step\n",
                "                        to_file = file + f_step\n",
                "                        if 0 <= to_file < 8:\n",
                "                            dest = to_rank * 8 + to_file\n",
                "                            uci = chess.Move(from_sq, dest, promotion=chess.Piece.from_symbol(p).piece_type).uci()\n",
                "                            move_to_idx[uci] = idx\n",
                "                            idx_to_move[idx] = uci\n",
                "                    idx += 1\n",
                "        return move_to_idx, idx_to_move\n",
                "    \n",
                "    def encode_move(self, move_uci):\n",
                "        idx = self.move_to_idx.get(move_uci, None)\n",
                "        if idx is not None:\n",
                "            return idx\n",
                "        if isinstance(move_uci, str) and len(move_uci) == 5 and move_uci[-1] == 'q':\n",
                "            return self.move_to_idx.get(move_uci[:4], None)\n",
                "        return None\n",
                "\n",
                "\n",
                "# ============================================================\n",
                "# BOARD ENCODER\n",
                "# ============================================================\n",
                "class BoardEncoder:\n",
                "    def __init__(self):\n",
                "        self.piece_map = {'P':0,'N':1,'B':2,'R':3,'Q':4,'K':5,'p':6,'n':7,'b':8,'r':9,'q':10,'k':11}\n",
                "    \n",
                "    def encode(self, board, prev_board=None):\n",
                "        tensor = np.zeros((32, 8, 8), dtype=np.float32)\n",
                "        for square, piece in board.piece_map().items():\n",
                "            channel = self.piece_map[piece.symbol()]\n",
                "            rank, file = chess.square_rank(square), chess.square_file(square)\n",
                "            tensor[channel][rank][file] = 1.0\n",
                "        if prev_board:\n",
                "            for square, piece in prev_board.piece_map().items():\n",
                "                channel = self.piece_map[piece.symbol()] + 12\n",
                "                rank, file = chess.square_rank(square), chess.square_file(square)\n",
                "                tensor[channel][rank][file] = 1.0\n",
                "        if board.turn == chess.WHITE: tensor[24, :, :] = 1.0\n",
                "        if board.has_kingside_castling_rights(chess.WHITE): tensor[25, :, :] = 1.0\n",
                "        if board.has_queenside_castling_rights(chess.WHITE): tensor[26, :, :] = 1.0\n",
                "        if board.has_kingside_castling_rights(chess.BLACK): tensor[27, :, :] = 1.0\n",
                "        if board.has_queenside_castling_rights(chess.BLACK): tensor[28, :, :] = 1.0\n",
                "        if board.ep_square:\n",
                "            rank, file = chess.square_rank(board.ep_square), chess.square_file(board.ep_square)\n",
                "            tensor[29][rank][file] = 1.0\n",
                "        if board.is_repetition(1): tensor[30, :, :] = 1.0\n",
                "        if board.is_repetition(2): tensor[31, :, :] = 1.0\n",
                "        return tensor\n",
                "\n",
                "\n",
                "# ============================================================\n",
                "# MODEL\n",
                "# ============================================================\n",
                "class ResidualBlock(nn.Module):\n",
                "    def __init__(self, num_channels):\n",
                "        super().__init__()\n",
                "        self.conv1 = nn.Conv2d(num_channels, num_channels, 3, padding=1, bias=False)\n",
                "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
                "        self.conv2 = nn.Conv2d(num_channels, num_channels, 3, padding=1, bias=False)\n",
                "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        residual = x\n",
                "        out = F.relu(self.bn1(self.conv1(x)))\n",
                "        out = self.bn2(self.conv2(out))\n",
                "        return F.relu(out + residual)\n",
                "\n",
                "class SmallResNet(nn.Module):\n",
                "    def __init__(self, num_res_blocks=6, num_channels=64, action_size=4672):\n",
                "        super().__init__()\n",
                "        self.conv_input = nn.Conv2d(32, num_channels, 3, padding=1, bias=False)\n",
                "        self.bn_input = nn.BatchNorm2d(num_channels)\n",
                "        self.res_blocks = nn.ModuleList([ResidualBlock(num_channels) for _ in range(num_res_blocks)])\n",
                "        self.policy_conv = nn.Conv2d(num_channels, 32, 1, bias=False)\n",
                "        self.policy_bn = nn.BatchNorm2d(32)\n",
                "        self.policy_fc = nn.Linear(32 * 8 * 8, action_size)\n",
                "        self.value_conv = nn.Conv2d(num_channels, 3, 1, bias=False)\n",
                "        self.value_bn = nn.BatchNorm2d(3)\n",
                "        self.value_fc1 = nn.Linear(3 * 8 * 8, 64)\n",
                "        self.value_fc2 = nn.Linear(64, 1)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = F.relu(self.bn_input(self.conv_input(x)))\n",
                "        for block in self.res_blocks:\n",
                "            x = block(x)\n",
                "        p = F.relu(self.policy_bn(self.policy_conv(x)))\n",
                "        p = p.view(-1, 32 * 8 * 8)\n",
                "        policy_logits = self.policy_fc(p)\n",
                "        v = F.relu(self.value_bn(self.value_conv(x)))\n",
                "        v = v.view(-1, 3 * 8 * 8)\n",
                "        v = F.relu(self.value_fc1(v))\n",
                "        value = torch.tanh(self.value_fc2(v))\n",
                "        return policy_logits, value\n",
                "\n",
                "\n",
                "# ============================================================\n",
                "# MCTS\n",
                "# ============================================================\n",
                "class MCTSNode:\n",
                "    def __init__(self, parent=None, move=None, prior=0.0):\n",
                "        self.parent = parent\n",
                "        self.move = move\n",
                "        self.prior = prior\n",
                "        self.children = {}\n",
                "        self.visit_count = 0\n",
                "        self.value_sum = 0.0\n",
                "    \n",
                "    def value(self):\n",
                "        return self.value_sum / self.visit_count if self.visit_count > 0 else 0.0\n",
                "    \n",
                "    def select_child(self, c_puct=1.5):\n",
                "        best_score, best_child = -float('inf'), None\n",
                "        for child in self.children.values():\n",
                "            q_value = -child.value() if child.visit_count > 0 else 0.0\n",
                "            u_value = c_puct * child.prior * np.sqrt(max(1, self.visit_count)) / (1 + child.visit_count)\n",
                "            score = q_value + u_value\n",
                "            if score > best_score:\n",
                "                best_score, best_child = score, child\n",
                "        return best_child\n",
                "    \n",
                "    def expand(self, moves_and_priors):\n",
                "        for move, prior in moves_and_priors.items():\n",
                "            if move not in self.children:\n",
                "                self.children[move] = MCTSNode(parent=self, move=move, prior=prior)\n",
                "    \n",
                "    def backpropagate(self, value):\n",
                "        node = self\n",
                "        while node:\n",
                "            node.visit_count += 1\n",
                "            node.value_sum += value\n",
                "            value = -value\n",
                "            node = node.parent\n",
                "\n",
                "class MCTS:\n",
                "    def __init__(self, model, move_encoder, board_encoder, c_puct=1.5, device='cpu'):\n",
                "        self.model = model\n",
                "        self.move_encoder = move_encoder\n",
                "        self.board_encoder = board_encoder\n",
                "        self.c_puct = c_puct\n",
                "        self.device = device\n",
                "    \n",
                "    def search(self, board, num_simulations, temperature=1.0, prev_board=None):\n",
                "        root = MCTSNode()\n",
                "        self._expand_node(root, board, prev_board)\n",
                "        \n",
                "        for _ in range(num_simulations):\n",
                "            node = root\n",
                "            search_board = board.copy()\n",
                "            search_prev_board = prev_board\n",
                "            \n",
                "            while len(node.children) > 0 and not search_board.is_game_over():\n",
                "                node = node.select_child(self.c_puct)\n",
                "                if node.move:\n",
                "                    search_prev_board = search_board.copy()\n",
                "                    search_board.push(node.move)\n",
                "            \n",
                "            if not search_board.is_game_over():\n",
                "                value = self._expand_node(node, search_board, search_prev_board)\n",
                "            else:\n",
                "                result = search_board.result()\n",
                "                if result == \"1-0\":\n",
                "                    value = -1.0 if search_board.turn == chess.BLACK else 1.0\n",
                "                elif result == \"0-1\":\n",
                "                    value = -1.0 if search_board.turn == chess.WHITE else 1.0\n",
                "                else:\n",
                "                    value = 0.0\n",
                "            \n",
                "            node.backpropagate(value)\n",
                "        \n",
                "        return self._get_action_probs(root, temperature)\n",
                "    \n",
                "    def _expand_node(self, node, board, prev_board):\n",
                "        state = self.board_encoder.encode(board, prev_board)\n",
                "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            policy_logits, value = self.model(state_tensor)\n",
                "        \n",
                "        policy = torch.softmax(policy_logits, dim=1).cpu().numpy()[0]\n",
                "        value = value.cpu().item()\n",
                "        \n",
                "        legal_moves = list(board.legal_moves)\n",
                "        moves_and_priors = {}\n",
                "        total_prior = 0.0\n",
                "        \n",
                "        for move in legal_moves:\n",
                "            move_idx = self.move_encoder.encode_move(move.uci())\n",
                "            prior = policy[move_idx] if move_idx is not None else 0.0\n",
                "            moves_and_priors[move] = prior\n",
                "            total_prior += prior\n",
                "        \n",
                "        if total_prior > 0:\n",
                "            moves_and_priors = {m: p/total_prior for m, p in moves_and_priors.items()}\n",
                "        else:\n",
                "            uniform = 1.0 / len(legal_moves)\n",
                "            moves_and_priors = {m: uniform for m in legal_moves}\n",
                "        \n",
                "        node.expand(moves_and_priors)\n",
                "        return value\n",
                "    \n",
                "    def _get_action_probs(self, root, temperature):\n",
                "        action_probs = np.zeros(4672, dtype=np.float32)\n",
                "        if len(root.children) == 0:\n",
                "            return action_probs\n",
                "        \n",
                "        moves = list(root.children.keys())\n",
                "        visits = np.array([root.children[m].visit_count for m in moves], dtype=np.float32)\n",
                "        \n",
                "        if temperature == 0:\n",
                "            probs = np.zeros(len(visits))\n",
                "            probs[np.argmax(visits)] = 1.0\n",
                "        else:\n",
                "            visits = visits ** (1.0 / temperature)\n",
                "            probs = visits / visits.sum()\n",
                "        \n",
                "        for move, prob in zip(moves, probs):\n",
                "            move_idx = self.move_encoder.encode_move(move.uci())\n",
                "            if move_idx is not None:\n",
                "                action_probs[move_idx] = prob\n",
                "        \n",
                "        return action_probs\n",
                "\n",
                "\n",
                "# ============================================================\n",
                "# REPLAY BUFFER\n",
                "# ============================================================\n",
                "class ReplayBuffer:\n",
                "    def __init__(self, max_size=100000):\n",
                "        self.buffer = deque(maxlen=max_size)\n",
                "    \n",
                "    def add_batch(self, samples):\n",
                "        self.buffer.extend(samples)\n",
                "    \n",
                "    def sample(self, batch_size):\n",
                "        return random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.buffer)\n",
                "\n",
                "\n",
                "# ============================================================\n",
                "# LOSS FUNCTION\n",
                "# ============================================================\n",
                "class AlphaZeroLoss(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.mse = nn.MSELoss()\n",
                "    \n",
                "    def forward(self, policy_pred, value_pred, policy_target, value_target):\n",
                "        log_policy = torch.log_softmax(policy_pred, dim=1)\n",
                "        policy_loss = -(policy_target * log_policy).sum(dim=1).mean()\n",
                "        value_loss = self.mse(value_pred, value_target)\n",
                "        return policy_loss + value_loss, policy_loss, value_loss\n",
                "\n",
                "print(\" All classes defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================\n",
                "SFT_CHECKPOINT = '/content/drive/MyDrive/models/sft_best.pth'  # ‚Üê UPDATE!\n",
                "SAVE_PATH = '/content/drive/MyDrive/models'\n",
                "\n",
                "# RL Hyperparameters\n",
                "RL_ITERATIONS = 100\n",
                "GAMES_PER_ITER = 30\n",
                "NUM_SIMULATIONS = 100\n",
                "BATCH_SIZE = 64\n",
                "LEARNING_RATE = 5e-5\n",
                "EVAL_INTERVAL = 5\n",
                "\n",
                "print(f\"üìä RL Configuration:\")\n",
                "print(f\"   MCTS simulations: {NUM_SIMULATIONS}\")\n",
                "print(f\"   Games per iteration: {GAMES_PER_ITER}\")\n",
                "print(f\"   Total iterations: {RL_ITERATIONS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load Model & SFT Checkpoint"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = SmallResNet(num_res_blocks=6, num_channels=64, action_size=4672).to(device)\n",
                "\n",
                "if os.path.exists(SFT_CHECKPOINT):\n",
                "    print(f\"üîÑ Loading SFT checkpoint: {SFT_CHECKPOINT}\")\n",
                "    checkpoint = torch.load(SFT_CHECKPOINT, map_location=device)\n",
                "    model.load_state_dict(checkpoint['model_state_dict'])\n",
                "    print(f\" SFT checkpoint loaded!\")\n",
                "else:\n",
                "    print(f\" No SFT checkpoint found, training from scratch\")\n",
                "\n",
                "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
                "criterion = AlphaZeroLoss()\n",
                "\n",
                "move_encoder = CanonicalMoveEncoder()\n",
                "board_encoder = BoardEncoder()\n",
                "mcts = MCTS(model, move_encoder, board_encoder, c_puct=1.5, device=device)\n",
                "replay_buffer = ReplayBuffer(max_size=100000)\n",
                "\n",
                "print(f\" RL trainer initialized\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Self-Play Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_self_play_game(mcts, num_simulations=100):\n",
                "    board = chess.Board()\n",
                "    samples = []\n",
                "    prev_board = None\n",
                "    move_count = 0\n",
                "    \n",
                "    while not board.is_game_over() and move_count < 200:\n",
                "        state = board_encoder.encode(board, prev_board)\n",
                "        temperature = 1.0 if move_count < 30 else 0.0\n",
                "        \n",
                "        policy_target = mcts.search(board, num_simulations, temperature, prev_board)\n",
                "        samples.append((state, policy_target, None, board.turn == chess.WHITE))\n",
                "        \n",
                "        # Select move\n",
                "        legal_moves = list(board.legal_moves)\n",
                "        legal_probs = []\n",
                "        for move in legal_moves:\n",
                "            move_idx = move_encoder.encode_move(move.uci())\n",
                "            legal_probs.append(policy_target[move_idx] if move_idx is not None else 0.0)\n",
                "        \n",
                "        if sum(legal_probs) > 0:\n",
                "            legal_probs = np.array(legal_probs, dtype=np.float64)\n",
                "            legal_probs = np.clip(legal_probs, 0.0, None)\n",
                "            legal_probs /= legal_probs.sum()\n",
                "            move_idx = np.random.choice(len(legal_moves), p=legal_probs)\n",
                "            selected_move = legal_moves[move_idx]\n",
                "        else:\n",
                "            selected_move = random.choice(legal_moves)\n",
                "        \n",
                "        prev_board = board.copy()\n",
                "        board.push(selected_move)\n",
                "        move_count += 1\n",
                "    \n",
                "    # Determine outcome\n",
                "    if move_count >= 200:\n",
                "        game_outcome = 0.0\n",
                "    else:\n",
                "        result = board.result()\n",
                "        game_outcome = 1.0 if result == \"1-0\" else (-1.0 if result == \"0-1\" else 0.0)\n",
                "    \n",
                "    # Fill in values\n",
                "    final_samples = []\n",
                "    for state, policy, _, was_white in samples:\n",
                "        value = game_outcome if was_white else -game_outcome\n",
                "        final_samples.append((state, policy, value))\n",
                "    \n",
                "    return final_samples, game_outcome\n",
                "\n",
                "print(\" Self-play function defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. RL Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs(SAVE_PATH, exist_ok=True)\n",
                "best_winrate = 0.0\n",
                "\n",
                "for iteration in range(1, RL_ITERATIONS + 1):\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Iteration {iteration}/{RL_ITERATIONS}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    # Self-play\n",
                "    print(f\"üéÆ Self-play: {GAMES_PER_ITER} games...\")\n",
                "    model.eval()\n",
                "    for _ in tqdm(range(GAMES_PER_ITER), desc=\"Self-play\"):\n",
                "        samples, _ = generate_self_play_game(mcts, NUM_SIMULATIONS)\n",
                "        replay_buffer.add_batch(samples)\n",
                "    \n",
                "    print(f\"üìä Buffer size: {len(replay_buffer)} samples\")\n",
                "    \n",
                "    # Training\n",
                "    print(f\"üèãÔ∏è Training on replay buffer...\")\n",
                "    model.train()\n",
                "    \n",
                "    for step in range(500):  # Train for 500 steps\n",
                "        batch = replay_buffer.sample(BATCH_SIZE)\n",
                "        states, policies, values = zip(*batch)\n",
                "        \n",
                "        states = torch.stack([torch.FloatTensor(s) for s in states]).to(device)\n",
                "        policies = torch.stack([torch.FloatTensor(p) for p in policies]).to(device)\n",
                "        values = torch.FloatTensor(values).unsqueeze(1).to(device)\n",
                "        \n",
                "        policy_pred, value_pred = model(states)\n",
                "        loss, p_loss, v_loss = criterion(policy_pred, value_pred, policies, values)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
                "        optimizer.step()\n",
                "        \n",
                "        if step % 100 == 0:\n",
                "            print(f\"  Step {step}/500: Loss={loss.item():.4f}\")\n",
                "    \n",
                "    # Save checkpoint\n",
                "    torch.save({\n",
                "        'model_state_dict': model.state_dict(),\n",
                "        'optimizer_state_dict': optimizer.state_dict(),\n",
                "        'architecture': {'action_size': 4672, 'num_res_blocks': 6, 'num_channels': 64},\n",
                "        'metadata': {'iteration': iteration, 'buffer_size': len(replay_buffer), 'stage': 'rl'}\n",
                "    }, f\"{SAVE_PATH}/rl_iter_{iteration}.pth\")\n",
                "    \n",
                "    print(f\"üíæ Checkpoint saved: rl_iter_{iteration}.pth\")\n",
                "\n",
                "print(f\"\\n RL Training complete!\")\n",
                "print(f\"üìÅ Final model: {SAVE_PATH}/rl_iter_{RL_ITERATIONS}.pth\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
