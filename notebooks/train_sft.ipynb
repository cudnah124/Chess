{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SFT Training - Supervised Fine-Tuning for Chess AI\n",
                "\n",
                "**Self-contained notebook** - All classes defined inline, no external imports needed!\n",
                "\n",
                "**Features:**\n",
                "- ‚úÖ Fixed 4672 action space (canonical encoding)\n",
                "- ‚úÖ Complete implementation in single notebook\n",
                "- ‚úÖ Google Colab ready\n",
                "- ‚úÖ Just upload PGN files and run!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch numpy python-chess tqdm -q\n",
                "\n",
                "# Mount Google Drive\n",
                "try:\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "except:\n",
                "    print(\"Not in Colab\")\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import numpy as np\n",
                "import chess\n",
                "import chess.pgn\n",
                "import os\n",
                "from tqdm import tqdm\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\" Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define All Classes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CANONICAL MOVE ENCODER - Fixed 4672 action space\n",
                "# ============================================================\n",
                "class CanonicalMoveEncoder:\n",
                "    def __init__(self):\n",
                "        self.move_to_idx, self.idx_to_move = self._build_canonical_map()\n",
                "        print(f\"üîí Canonical move map: {len(self.move_to_idx)} moves\")\n",
                "    \n",
                "    def _build_canonical_map(self):\n",
                "        move_to_idx, idx_to_move = {}, {}\n",
                "        idx = 0\n",
                "        \n",
                "        # Queen moves (3584)\n",
                "        directions = [(1,0),(1,1),(0,1),(-1,1),(-1,0),(-1,-1),(0,-1),(1,-1)]\n",
                "        for from_sq in range(64):\n",
                "            for d_r, d_f in directions:\n",
                "                for dist in range(1, 8):\n",
                "                    to_rank = (from_sq // 8) + d_r * dist\n",
                "                    to_file = (from_sq % 8) + d_f * dist\n",
                "                    if 0 <= to_rank < 8 and 0 <= to_file < 8:\n",
                "                        dest = to_rank * 8 + to_file\n",
                "                        uci = chess.Move(from_sq, dest).uci()\n",
                "                        move_to_idx[uci] = idx\n",
                "                        idx_to_move[idx] = uci\n",
                "                    idx += 1\n",
                "        \n",
                "        # Knight moves (512)\n",
                "        knight_moves = [(2,1),(1,2),(-1,2),(-2,1),(-2,-1),(-1,-2),(1,-2),(2,-1)]\n",
                "        for from_sq in range(64):\n",
                "            for d_r, d_f in knight_moves:\n",
                "                to_rank = (from_sq // 8) + d_r\n",
                "                to_file = (from_sq % 8) + d_f\n",
                "                if 0 <= to_rank < 8 and 0 <= to_file < 8:\n",
                "                    dest = to_rank * 8 + to_file\n",
                "                    uci = chess.Move(from_sq, dest).uci()\n",
                "                    move_to_idx[uci] = idx\n",
                "                    idx_to_move[idx] = uci\n",
                "                idx += 1\n",
                "        \n",
                "        # Underpromotions (576)\n",
                "        for from_sq in range(64):\n",
                "            rank = from_sq // 8\n",
                "            file = from_sq % 8\n",
                "            rank_step = 1 if rank == 6 else (-1 if rank == 1 else 0)\n",
                "            \n",
                "            for f_step in [0, -1, 1]:\n",
                "                for p in ['r', 'b', 'n']:\n",
                "                    if rank_step != 0:\n",
                "                        to_rank = rank + rank_step\n",
                "                        to_file = file + f_step\n",
                "                        if 0 <= to_file < 8:\n",
                "                            dest = to_rank * 8 + to_file\n",
                "                            uci = chess.Move(from_sq, dest, promotion=chess.Piece.from_symbol(p).piece_type).uci()\n",
                "                            move_to_idx[uci] = idx\n",
                "                            idx_to_move[idx] = uci\n",
                "                    idx += 1\n",
                "        \n",
                "        return move_to_idx, idx_to_move\n",
                "    \n",
                "    def encode_move(self, move_uci):\n",
                "        idx = self.move_to_idx.get(move_uci, None)\n",
                "        if idx is not None:\n",
                "            return idx\n",
                "        # Queen promotion fallback\n",
                "        if isinstance(move_uci, str) and len(move_uci) == 5 and move_uci[-1] == 'q':\n",
                "            return self.move_to_idx.get(move_uci[:4], None)\n",
                "        return None\n",
                "\n",
                "\n",
                "# ============================================================\n",
                "# BOARD ENCODER - 32-channel tensor representation\n",
                "# ============================================================\n",
                "class BoardEncoder:\n",
                "    def __init__(self):\n",
                "        self.piece_map = {\n",
                "            'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
                "            'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
                "        }\n",
                "    \n",
                "    def encode(self, board, prev_board=None):\n",
                "        tensor = np.zeros((32, 8, 8), dtype=np.float32)\n",
                "        \n",
                "        # Current pieces (0-11)\n",
                "        for square, piece in board.piece_map().items():\n",
                "            channel = self.piece_map[piece.symbol()]\n",
                "            rank, file = chess.square_rank(square), chess.square_file(square)\n",
                "            tensor[channel][rank][file] = 1.0\n",
                "        \n",
                "        # Previous position (12-23)\n",
                "        if prev_board:\n",
                "            for square, piece in prev_board.piece_map().items():\n",
                "                channel = self.piece_map[piece.symbol()] + 12\n",
                "                rank, file = chess.square_rank(square), chess.square_file(square)\n",
                "                tensor[channel][rank][file] = 1.0\n",
                "        \n",
                "        # Metadata (24-31)\n",
                "        if board.turn == chess.WHITE: tensor[24, :, :] = 1.0\n",
                "        if board.has_kingside_castling_rights(chess.WHITE): tensor[25, :, :] = 1.0\n",
                "        if board.has_queenside_castling_rights(chess.WHITE): tensor[26, :, :] = 1.0\n",
                "        if board.has_kingside_castling_rights(chess.BLACK): tensor[27, :, :] = 1.0\n",
                "        if board.has_queenside_castling_rights(chess.BLACK): tensor[28, :, :] = 1.0\n",
                "        if board.ep_square:\n",
                "            rank, file = chess.square_rank(board.ep_square), chess.square_file(board.ep_square)\n",
                "            tensor[29][rank][file] = 1.0\n",
                "        if board.is_repetition(1): tensor[30, :, :] = 1.0\n",
                "        if board.is_repetition(2): tensor[31, :, :] = 1.0\n",
                "        \n",
                "        return tensor\n",
                "\n",
                "\n",
                "# ============================================================\n",
                "# MODEL ARCHITECTURE - SmallResNet\n",
                "# ============================================================\n",
                "class ResidualBlock(nn.Module):\n",
                "    def __init__(self, num_channels):\n",
                "        super().__init__()\n",
                "        self.conv1 = nn.Conv2d(num_channels, num_channels, 3, padding=1, bias=False)\n",
                "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
                "        self.conv2 = nn.Conv2d(num_channels, num_channels, 3, padding=1, bias=False)\n",
                "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        residual = x\n",
                "        out = F.relu(self.bn1(self.conv1(x)))\n",
                "        out = self.bn2(self.conv2(out))\n",
                "        out += residual\n",
                "        return F.relu(out)\n",
                "\n",
                "class SmallResNet(nn.Module):\n",
                "    def __init__(self, num_res_blocks=6, num_channels=64, action_size=4672):\n",
                "        super().__init__()\n",
                "        self.conv_input = nn.Conv2d(32, num_channels, 3, padding=1, bias=False)\n",
                "        self.bn_input = nn.BatchNorm2d(num_channels)\n",
                "        self.res_blocks = nn.ModuleList([ResidualBlock(num_channels) for _ in range(num_res_blocks)])\n",
                "        \n",
                "        # Policy head\n",
                "        self.policy_conv = nn.Conv2d(num_channels, 32, 1, bias=False)\n",
                "        self.policy_bn = nn.BatchNorm2d(32)\n",
                "        self.policy_fc = nn.Linear(32 * 8 * 8, action_size)\n",
                "        \n",
                "        # Value head\n",
                "        self.value_conv = nn.Conv2d(num_channels, 3, 1, bias=False)\n",
                "        self.value_bn = nn.BatchNorm2d(3)\n",
                "        self.value_fc1 = nn.Linear(3 * 8 * 8, 64)\n",
                "        self.value_fc2 = nn.Linear(64, 1)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = F.relu(self.bn_input(self.conv_input(x)))\n",
                "        for block in self.res_blocks:\n",
                "            x = block(x)\n",
                "        \n",
                "        # Policy\n",
                "        p = F.relu(self.policy_bn(self.policy_conv(x)))\n",
                "        p = p.view(-1, 32 * 8 * 8)\n",
                "        policy_logits = self.policy_fc(p)\n",
                "        \n",
                "        # Value\n",
                "        v = F.relu(self.value_bn(self.value_conv(x)))\n",
                "        v = v.view(-1, 3 * 8 * 8)\n",
                "        v = F.relu(self.value_fc1(v))\n",
                "        value = torch.tanh(self.value_fc2(v))\n",
                "        \n",
                "        return policy_logits, value\n",
                "\n",
                "\n",
                "# ============================================================\n",
                "# LOSS FUNCTION\n",
                "# ============================================================\n",
                "class AlphaZeroLoss(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.mse = nn.MSELoss()\n",
                "    \n",
                "    def forward(self, policy_pred, value_pred, policy_target, value_target):\n",
                "        log_policy = torch.log_softmax(policy_pred, dim=1)\n",
                "        policy_loss = -(policy_target * log_policy).sum(dim=1).mean()\n",
                "        value_loss = self.mse(value_pred, value_target)\n",
                "        return policy_loss + value_loss, policy_loss, value_loss\n",
                "\n",
                "\n",
                "# ============================================================\n",
                "# DATASET\n",
                "# ============================================================\n",
                "class ChessPGNDataset(Dataset):\n",
                "    def __init__(self, pgn_paths, max_games=None):\n",
                "        self.move_encoder = CanonicalMoveEncoder()\n",
                "        self.board_encoder = BoardEncoder()\n",
                "        self.samples = []\n",
                "        \n",
                "        for pgn_path in pgn_paths:\n",
                "            print(f\"\\nüìÇ Processing: {pgn_path}\")\n",
                "            self._process_pgn(pgn_path, max_games)\n",
                "        \n",
                "        print(f\"\\n Total samples: {len(self.samples)}\")\n",
                "    \n",
                "    def _process_pgn(self, pgn_path, max_games):\n",
                "        games_processed = 0\n",
                "        \n",
                "        with open(pgn_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
                "            pbar = tqdm(desc=\"Processing games\")\n",
                "            \n",
                "            while True:\n",
                "                if max_games and games_processed >= max_games:\n",
                "                    break\n",
                "                \n",
                "                game = chess.pgn.read_game(f)\n",
                "                if game is None:\n",
                "                    break\n",
                "                \n",
                "                result = game.headers.get(\"Result\", \"*\")\n",
                "                if result == \"1-0\":\n",
                "                    game_outcome = 1.0\n",
                "                elif result == \"0-1\":\n",
                "                    game_outcome = -1.0\n",
                "                elif result == \"1/2-1/2\":\n",
                "                    game_outcome = 0.0\n",
                "                else:\n",
                "                    continue\n",
                "                \n",
                "                board = game.board()\n",
                "                prev_board = None\n",
                "                \n",
                "                for move in game.mainline_moves():\n",
                "                    state = self.board_encoder.encode(board, prev_board)\n",
                "                    move_idx = self.move_encoder.encode_move(move.uci())\n",
                "                    \n",
                "                    if move_idx is not None:\n",
                "                        policy_target = np.zeros(4672, dtype=np.float32)\n",
                "                        policy_target[move_idx] = 1.0\n",
                "                        value_target = game_outcome if board.turn == chess.WHITE else -game_outcome\n",
                "                        self.samples.append((state, policy_target, value_target))\n",
                "                    \n",
                "                    prev_board = board.copy()\n",
                "                    board.push(move)\n",
                "                \n",
                "                games_processed += 1\n",
                "                pbar.update(1)\n",
                "                pbar.set_postfix({'samples': len(self.samples)})\n",
                "            \n",
                "            pbar.close()\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        state, policy, value = self.samples[idx]\n",
                "        return {\n",
                "            'state': torch.FloatTensor(state),\n",
                "            'policy': torch.FloatTensor(policy),\n",
                "            'value': torch.FloatTensor([value])\n",
                "        }\n",
                "\n",
                "print(\" All classes defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION - Update paths for your data!\n",
                "# ============================================================\n",
                "PGN_PATHS = [\n",
                "    '/content/drive/MyDrive/data/games_2000.pgn',  # ‚Üê UPDATE THESE!\n",
                "    '/content/drive/MyDrive/data/games_2001.pgn',\n",
                "]\n",
                "\n",
                "SAVE_PATH = '/content/drive/MyDrive/models'  # ‚Üê UPDATE THIS!\n",
                "\n",
                "# Hyperparameters\n",
                "MAX_GAMES = 8000\n",
                "BATCH_SIZE = 256\n",
                "EPOCHS = 20\n",
                "LEARNING_RATE = 1e-3\n",
                "WEIGHT_DECAY = 1e-4\n",
                "\n",
                "print(f\"üìä Configuration:\")\n",
                "print(f\"   Batch size: {BATCH_SIZE}\")\n",
                "print(f\"   Epochs: {EPOCHS}\")\n",
                "print(f\"   Learning rate: {LEARNING_RATE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = ChessPGNDataset(PGN_PATHS, max_games=MAX_GAMES)\n",
                "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
                "\n",
                "print(f\" Dataset ready: {len(dataset)} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Initialize Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = SmallResNet(num_res_blocks=6, num_channels=64, action_size=4672).to(device)\n",
                "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
                "criterion = AlphaZeroLoss()\n",
                "\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "print(f\" Model initialized\")\n",
                "print(f\"   Parameters: {total_params:,}\")\n",
                "print(f\"   Policy output: 4672\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs(SAVE_PATH, exist_ok=True)\n",
                "best_loss = float('inf')\n",
                "\n",
                "for epoch in range(1, EPOCHS + 1):\n",
                "    model.train()\n",
                "    total_loss = 0.0\n",
                "    \n",
                "    pbar = tqdm(loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
                "    \n",
                "    for batch in pbar:\n",
                "        states = batch['state'].to(device)\n",
                "        policy_targets = batch['policy'].to(device)\n",
                "        value_targets = batch['value'].to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        policy_pred, value_pred = model(states)\n",
                "        \n",
                "        loss, p_loss, v_loss = criterion(policy_pred, value_pred, policy_targets, value_targets)\n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'p': f'{p_loss.item():.4f}', 'v': f'{v_loss.item():.4f}'})\n",
                "    \n",
                "    avg_loss = total_loss / len(loader)\n",
                "    print(f\"\\nEpoch {epoch} - Avg Loss: {avg_loss:.4f}\")\n",
                "    \n",
                "    # Save checkpoint\n",
                "    torch.save({\n",
                "        'model_state_dict': model.state_dict(),\n",
                "        'optimizer_state_dict': optimizer.state_dict(),\n",
                "        'architecture': {'action_size': 4672, 'num_res_blocks': 6, 'num_channels': 64},\n",
                "        'metadata': {'epoch': epoch, 'loss': avg_loss, 'stage': 'sft'}\n",
                "    }, f\"{SAVE_PATH}/sft_epoch_{epoch}.pth\")\n",
                "    \n",
                "    if avg_loss < best_loss:\n",
                "        best_loss = avg_loss\n",
                "        torch.save({\n",
                "            'model_state_dict': model.state_dict(),\n",
                "            'optimizer_state_dict': optimizer.state_dict(),\n",
                "            'architecture': {'action_size': 4672, 'num_res_blocks': 6, 'num_channels': 64},\n",
                "            'metadata': {'epoch': epoch, 'loss': avg_loss, 'stage': 'sft'}\n",
                "        }, f\"{SAVE_PATH}/sft_best.pth\")\n",
                "        print(f\"üåü New best model saved!\")\n",
                "\n",
                "print(f\"\\n Training complete! Best loss: {best_loss:.4f}\")\n",
                "print(f\"üìÅ Model saved to: {SAVE_PATH}/sft_best.pth\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
